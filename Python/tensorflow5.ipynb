{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "E1qLnvy0WDmv",
        "outputId": "618655d5-e84e-480d-dd65-1dd7f0f5f01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 21 (delta 3), reused 20 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (21/21), 460.95 KiB | 12.46 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
              "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
              "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
              "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
              "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
              "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
              "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
              "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
              "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
              "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
              "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
              "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
              "\n",
              "      11  12  \n",
              "0      5   1  \n",
              "1      5   1  \n",
              "2      5   1  \n",
              "3      6   1  \n",
              "4      5   1  \n",
              "...   ..  ..  \n",
              "6492   6   0  \n",
              "6493   5   0  \n",
              "6494   6   0  \n",
              "6495   7   0  \n",
              "6496   6   0  \n",
              "\n",
              "[6497 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d00eab6-a47e-4597-8d5a-9df706bb9658\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6492</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.039</td>\n",
              "      <td>24.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.99114</td>\n",
              "      <td>3.27</td>\n",
              "      <td>0.50</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6493</th>\n",
              "      <td>6.6</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.36</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.047</td>\n",
              "      <td>57.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6494</th>\n",
              "      <td>6.5</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.041</td>\n",
              "      <td>30.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>0.99254</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6495</th>\n",
              "      <td>5.5</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.30</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.022</td>\n",
              "      <td>20.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>0.98869</td>\n",
              "      <td>3.34</td>\n",
              "      <td>0.38</td>\n",
              "      <td>12.8</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6496</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.020</td>\n",
              "      <td>22.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.98941</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.32</td>\n",
              "      <td>11.8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6497 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d00eab6-a47e-4597-8d5a-9df706bb9658')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d00eab6-a47e-4597-8d5a-9df706bb9658 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d00eab6-a47e-4597-8d5a-9df706bb9658');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40e13643-efd7-4355-b138-a1297b69c0d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40e13643-efd7-4355-b138-a1297b69c0d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40e13643-efd7-4355-b138-a1297b69c0d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6497,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2964337577998153,\n        \"min\": 3.8,\n        \"max\": 15.9,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          7.15,\n          8.1,\n          7.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16463647408467877,\n        \"min\": 0.08,\n        \"max\": 1.58,\n        \"num_unique_values\": 187,\n        \"samples\": [\n          0.405,\n          0.21,\n          0.695\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14531786489759155,\n        \"min\": 0.0,\n        \"max\": 1.66,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          0.1,\n          0.6,\n          0.37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.757803743147418,\n        \"min\": 0.6,\n        \"max\": 65.8,\n        \"num_unique_values\": 316,\n        \"samples\": [\n          18.95,\n          3.2,\n          9.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03503360137245907,\n        \"min\": 0.009,\n        \"max\": 0.611,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          0.089,\n          0.217,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.7493997720025,\n        \"min\": 1.0,\n        \"max\": 289.0,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          77.5,\n          65.0,\n          128.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56.521854522630285,\n        \"min\": 6.0,\n        \"max\": 440.0,\n        \"num_unique_values\": 276,\n        \"samples\": [\n          14.0,\n          149.0,\n          227.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0029986730037190393,\n        \"min\": 0.98711,\n        \"max\": 1.03898,\n        \"num_unique_values\": 998,\n        \"samples\": [\n          0.9918,\n          0.99412,\n          0.99484\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16078720210398764,\n        \"min\": 2.72,\n        \"max\": 4.01,\n        \"num_unique_values\": 108,\n        \"samples\": [\n          3.74,\n          3.17,\n          3.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14880587361449027,\n        \"min\": 0.22,\n        \"max\": 2.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          1.11,\n          1.56,\n          0.46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 10,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.192711748870993,\n        \"min\": 8.0,\n        \"max\": 14.9,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          10.9333333333333,\n          9.7,\n          10.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 11,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 9,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5,\n          6,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 12,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# 깃허브에 준비된 데이터를 가져옵니다.\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "# 와인 데이터를 불러옵니다.\n",
        "df = pd.read_csv('./data/wine.csv', header=None)\n",
        "\n",
        "# 데이터를 미리 보겠습니다.\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
        "X = df.iloc[:,0:12]\n",
        "y = df.iloc[:,12]"
      ],
      "metadata": {
        "id": "xskn3r9ZWHNO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습셋과 테스트셋으로 나눕니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "# 모델 구조를 설정합니다.\n",
        "model = Sequential()\n",
        "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델을 실행합니다.\n",
        "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTa6J_cTWMlW",
        "outputId": "2ee8029b-be61-4945-f373-6150bb0dc3d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 30)                390       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 12)                372       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 875 (3.42 KB)\n",
            "Trainable params: 875 (3.42 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 2s 36ms/step - loss: 20.6106 - accuracy: 0.2510 - val_loss: 15.1978 - val_accuracy: 0.2338\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 10.3912 - accuracy: 0.2510 - val_loss: 5.7149 - val_accuracy: 0.2338\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3.8378 - accuracy: 0.2510 - val_loss: 1.7785 - val_accuracy: 0.2346\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.8272 - accuracy: 0.5810 - val_loss: 0.2973 - val_accuracy: 0.8769\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3064 - accuracy: 0.8709 - val_loss: 0.2824 - val_accuracy: 0.8792\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3120 - accuracy: 0.8740 - val_loss: 0.2783 - val_accuracy: 0.8923\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2976 - accuracy: 0.8881 - val_loss: 0.2470 - val_accuracy: 0.9115\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2565 - accuracy: 0.9135 - val_loss: 0.2085 - val_accuracy: 0.9315\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2268 - accuracy: 0.9243 - val_loss: 0.2107 - val_accuracy: 0.9315\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2177 - accuracy: 0.9264 - val_loss: 0.2002 - val_accuracy: 0.9338\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2165 - accuracy: 0.9258 - val_loss: 0.1982 - val_accuracy: 0.9331\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2127 - accuracy: 0.9276 - val_loss: 0.1978 - val_accuracy: 0.9323\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2111 - accuracy: 0.9279 - val_loss: 0.1946 - val_accuracy: 0.9338\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2089 - accuracy: 0.9279 - val_loss: 0.1920 - val_accuracy: 0.9354\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9279 - val_loss: 0.1920 - val_accuracy: 0.9338\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2057 - accuracy: 0.9284 - val_loss: 0.1897 - val_accuracy: 0.9354\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2044 - accuracy: 0.9287 - val_loss: 0.1889 - val_accuracy: 0.9362\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2032 - accuracy: 0.9292 - val_loss: 0.1878 - val_accuracy: 0.9331\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2018 - accuracy: 0.9284 - val_loss: 0.1859 - val_accuracy: 0.9362\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2004 - accuracy: 0.9292 - val_loss: 0.1857 - val_accuracy: 0.9338\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1991 - accuracy: 0.9289 - val_loss: 0.1840 - val_accuracy: 0.9354\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1984 - accuracy: 0.9287 - val_loss: 0.1827 - val_accuracy: 0.9377\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1971 - accuracy: 0.9289 - val_loss: 0.1825 - val_accuracy: 0.9346\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1962 - accuracy: 0.9294 - val_loss: 0.1818 - val_accuracy: 0.9346\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1951 - accuracy: 0.9289 - val_loss: 0.1800 - val_accuracy: 0.9377\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1941 - accuracy: 0.9287 - val_loss: 0.1795 - val_accuracy: 0.9346\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1922 - accuracy: 0.9279 - val_loss: 0.1775 - val_accuracy: 0.9385\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1908 - accuracy: 0.9294 - val_loss: 0.1768 - val_accuracy: 0.9331\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1884 - accuracy: 0.9279 - val_loss: 0.1745 - val_accuracy: 0.9377\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1871 - accuracy: 0.9281 - val_loss: 0.1740 - val_accuracy: 0.9323\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1854 - accuracy: 0.9294 - val_loss: 0.1711 - val_accuracy: 0.9385\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1852 - accuracy: 0.9281 - val_loss: 0.1692 - val_accuracy: 0.9385\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1848 - accuracy: 0.9284 - val_loss: 0.1729 - val_accuracy: 0.9308\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1838 - accuracy: 0.9302 - val_loss: 0.1682 - val_accuracy: 0.9392\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1833 - accuracy: 0.9287 - val_loss: 0.1661 - val_accuracy: 0.9377\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1791 - accuracy: 0.9294 - val_loss: 0.1644 - val_accuracy: 0.9392\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1776 - accuracy: 0.9299 - val_loss: 0.1644 - val_accuracy: 0.9377\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1766 - accuracy: 0.9302 - val_loss: 0.1624 - val_accuracy: 0.9408\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1754 - accuracy: 0.9317 - val_loss: 0.1644 - val_accuracy: 0.9362\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1754 - accuracy: 0.9310 - val_loss: 0.1602 - val_accuracy: 0.9415\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1746 - accuracy: 0.9307 - val_loss: 0.1633 - val_accuracy: 0.9377\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1738 - accuracy: 0.9305 - val_loss: 0.1580 - val_accuracy: 0.9408\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1720 - accuracy: 0.9310 - val_loss: 0.1569 - val_accuracy: 0.9415\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1707 - accuracy: 0.9315 - val_loss: 0.1573 - val_accuracy: 0.9392\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1691 - accuracy: 0.9317 - val_loss: 0.1550 - val_accuracy: 0.9415\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1684 - accuracy: 0.9323 - val_loss: 0.1543 - val_accuracy: 0.9423\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1670 - accuracy: 0.9330 - val_loss: 0.1535 - val_accuracy: 0.9423\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1653 - accuracy: 0.9353 - val_loss: 0.1522 - val_accuracy: 0.9431\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1644 - accuracy: 0.9356 - val_loss: 0.1525 - val_accuracy: 0.9415\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1635 - accuracy: 0.9346 - val_loss: 0.1502 - val_accuracy: 0.9431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 결과를 출력합니다.\n",
        "score=model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvSeHCFeWOhV",
        "outputId": "a0c4d0e1-8ff0-4cab-f5ac-4f58539e0e60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9562\n",
            "Test accuracy: 0.9561538696289062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 깃허브에 준비된 데이터를 가져옵니다. 앞에서 이미 데이터를 가져왔으므로 주석 처리합니다. 2번 예제만 별도 실행 시 주석을 해제한 후 실행해주세요.\n",
        "# !git clone https://github.com/taehojo/data.git\n",
        "\n",
        "# 와인 데이터를 불러옵니다.\n",
        "df = pd.read_csv('./data/wine.csv', header=None)\n",
        "\n",
        "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
        "X = df.iloc[:,0:12]\n",
        "y = df.iloc[:,12]\n",
        "\n",
        "# 학습셋과 테스트셋으로 나눕니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "# 모델 구조를 설정합니다.\n",
        "model = Sequential()\n",
        "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tsMmB9BWRqg",
        "outputId": "c5a29143-11bd-465b-c72e-807945212ea2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 30)                390       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 12)                372       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 875 (3.42 KB)\n",
            "Trainable params: 875 (3.42 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장의 조건을 설정합니다.\n",
        "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
        "\n",
        "# 모델을 실행합니다.\n",
        "history=model.fit(X_train, y_train, epochs=500, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZO3fnBsWUrg",
        "outputId": "3a7439bf-881b-4dcd-e292-050e05829f06"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: saving model to ./data/model/all/01-0.9800.hdf5\n",
            "\n",
            "Epoch 2: saving model to ./data/model/all/02-0.9808.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: saving model to ./data/model/all/03-0.9792.hdf5\n",
            "\n",
            "Epoch 4: saving model to ./data/model/all/04-0.9800.hdf5\n",
            "\n",
            "Epoch 5: saving model to ./data/model/all/05-0.9785.hdf5\n",
            "\n",
            "Epoch 6: saving model to ./data/model/all/06-0.9808.hdf5\n",
            "\n",
            "Epoch 7: saving model to ./data/model/all/07-0.9785.hdf5\n",
            "\n",
            "Epoch 8: saving model to ./data/model/all/08-0.9792.hdf5\n",
            "\n",
            "Epoch 9: saving model to ./data/model/all/09-0.9792.hdf5\n",
            "\n",
            "Epoch 10: saving model to ./data/model/all/10-0.9792.hdf5\n",
            "\n",
            "Epoch 11: saving model to ./data/model/all/11-0.9785.hdf5\n",
            "\n",
            "Epoch 12: saving model to ./data/model/all/12-0.9815.hdf5\n",
            "\n",
            "Epoch 13: saving model to ./data/model/all/13-0.9792.hdf5\n",
            "\n",
            "Epoch 14: saving model to ./data/model/all/14-0.9815.hdf5\n",
            "\n",
            "Epoch 15: saving model to ./data/model/all/15-0.9800.hdf5\n",
            "\n",
            "Epoch 16: saving model to ./data/model/all/16-0.9808.hdf5\n",
            "\n",
            "Epoch 17: saving model to ./data/model/all/17-0.9792.hdf5\n",
            "\n",
            "Epoch 18: saving model to ./data/model/all/18-0.9800.hdf5\n",
            "\n",
            "Epoch 19: saving model to ./data/model/all/19-0.9808.hdf5\n",
            "\n",
            "Epoch 20: saving model to ./data/model/all/20-0.9800.hdf5\n",
            "\n",
            "Epoch 21: saving model to ./data/model/all/21-0.9800.hdf5\n",
            "\n",
            "Epoch 22: saving model to ./data/model/all/22-0.9815.hdf5\n",
            "\n",
            "Epoch 23: saving model to ./data/model/all/23-0.9800.hdf5\n",
            "\n",
            "Epoch 24: saving model to ./data/model/all/24-0.9800.hdf5\n",
            "\n",
            "Epoch 25: saving model to ./data/model/all/25-0.9800.hdf5\n",
            "\n",
            "Epoch 26: saving model to ./data/model/all/26-0.9800.hdf5\n",
            "\n",
            "Epoch 27: saving model to ./data/model/all/27-0.9815.hdf5\n",
            "\n",
            "Epoch 28: saving model to ./data/model/all/28-0.9777.hdf5\n",
            "\n",
            "Epoch 29: saving model to ./data/model/all/29-0.9831.hdf5\n",
            "\n",
            "Epoch 30: saving model to ./data/model/all/30-0.9800.hdf5\n",
            "\n",
            "Epoch 31: saving model to ./data/model/all/31-0.9808.hdf5\n",
            "\n",
            "Epoch 32: saving model to ./data/model/all/32-0.9823.hdf5\n",
            "\n",
            "Epoch 33: saving model to ./data/model/all/33-0.9831.hdf5\n",
            "\n",
            "Epoch 34: saving model to ./data/model/all/34-0.9808.hdf5\n",
            "\n",
            "Epoch 35: saving model to ./data/model/all/35-0.9823.hdf5\n",
            "\n",
            "Epoch 36: saving model to ./data/model/all/36-0.9823.hdf5\n",
            "\n",
            "Epoch 37: saving model to ./data/model/all/37-0.9815.hdf5\n",
            "\n",
            "Epoch 38: saving model to ./data/model/all/38-0.9808.hdf5\n",
            "\n",
            "Epoch 39: saving model to ./data/model/all/39-0.9715.hdf5\n",
            "\n",
            "Epoch 40: saving model to ./data/model/all/40-0.9792.hdf5\n",
            "\n",
            "Epoch 41: saving model to ./data/model/all/41-0.9815.hdf5\n",
            "\n",
            "Epoch 42: saving model to ./data/model/all/42-0.9792.hdf5\n",
            "\n",
            "Epoch 43: saving model to ./data/model/all/43-0.9815.hdf5\n",
            "\n",
            "Epoch 44: saving model to ./data/model/all/44-0.9808.hdf5\n",
            "\n",
            "Epoch 45: saving model to ./data/model/all/45-0.9808.hdf5\n",
            "\n",
            "Epoch 46: saving model to ./data/model/all/46-0.9800.hdf5\n",
            "\n",
            "Epoch 47: saving model to ./data/model/all/47-0.9800.hdf5\n",
            "\n",
            "Epoch 48: saving model to ./data/model/all/48-0.9785.hdf5\n",
            "\n",
            "Epoch 49: saving model to ./data/model/all/49-0.9815.hdf5\n",
            "\n",
            "Epoch 50: saving model to ./data/model/all/50-0.9808.hdf5\n",
            "\n",
            "Epoch 51: saving model to ./data/model/all/51-0.9800.hdf5\n",
            "\n",
            "Epoch 52: saving model to ./data/model/all/52-0.9831.hdf5\n",
            "\n",
            "Epoch 53: saving model to ./data/model/all/53-0.9823.hdf5\n",
            "\n",
            "Epoch 54: saving model to ./data/model/all/54-0.9815.hdf5\n",
            "\n",
            "Epoch 55: saving model to ./data/model/all/55-0.9800.hdf5\n",
            "\n",
            "Epoch 56: saving model to ./data/model/all/56-0.9831.hdf5\n",
            "\n",
            "Epoch 57: saving model to ./data/model/all/57-0.9792.hdf5\n",
            "\n",
            "Epoch 58: saving model to ./data/model/all/58-0.9831.hdf5\n",
            "\n",
            "Epoch 59: saving model to ./data/model/all/59-0.9831.hdf5\n",
            "\n",
            "Epoch 60: saving model to ./data/model/all/60-0.9800.hdf5\n",
            "\n",
            "Epoch 61: saving model to ./data/model/all/61-0.9800.hdf5\n",
            "\n",
            "Epoch 62: saving model to ./data/model/all/62-0.9815.hdf5\n",
            "\n",
            "Epoch 63: saving model to ./data/model/all/63-0.9838.hdf5\n",
            "\n",
            "Epoch 64: saving model to ./data/model/all/64-0.9800.hdf5\n",
            "\n",
            "Epoch 65: saving model to ./data/model/all/65-0.9746.hdf5\n",
            "\n",
            "Epoch 66: saving model to ./data/model/all/66-0.9808.hdf5\n",
            "\n",
            "Epoch 67: saving model to ./data/model/all/67-0.9815.hdf5\n",
            "\n",
            "Epoch 68: saving model to ./data/model/all/68-0.9831.hdf5\n",
            "\n",
            "Epoch 69: saving model to ./data/model/all/69-0.9815.hdf5\n",
            "\n",
            "Epoch 70: saving model to ./data/model/all/70-0.9808.hdf5\n",
            "\n",
            "Epoch 71: saving model to ./data/model/all/71-0.9815.hdf5\n",
            "\n",
            "Epoch 72: saving model to ./data/model/all/72-0.9792.hdf5\n",
            "\n",
            "Epoch 73: saving model to ./data/model/all/73-0.9823.hdf5\n",
            "\n",
            "Epoch 74: saving model to ./data/model/all/74-0.9808.hdf5\n",
            "\n",
            "Epoch 75: saving model to ./data/model/all/75-0.9823.hdf5\n",
            "\n",
            "Epoch 76: saving model to ./data/model/all/76-0.9815.hdf5\n",
            "\n",
            "Epoch 77: saving model to ./data/model/all/77-0.9815.hdf5\n",
            "\n",
            "Epoch 78: saving model to ./data/model/all/78-0.9815.hdf5\n",
            "\n",
            "Epoch 79: saving model to ./data/model/all/79-0.9808.hdf5\n",
            "\n",
            "Epoch 80: saving model to ./data/model/all/80-0.9823.hdf5\n",
            "\n",
            "Epoch 81: saving model to ./data/model/all/81-0.9831.hdf5\n",
            "\n",
            "Epoch 82: saving model to ./data/model/all/82-0.9792.hdf5\n",
            "\n",
            "Epoch 83: saving model to ./data/model/all/83-0.9815.hdf5\n",
            "\n",
            "Epoch 84: saving model to ./data/model/all/84-0.9808.hdf5\n",
            "\n",
            "Epoch 85: saving model to ./data/model/all/85-0.9823.hdf5\n",
            "\n",
            "Epoch 86: saving model to ./data/model/all/86-0.9808.hdf5\n",
            "\n",
            "Epoch 87: saving model to ./data/model/all/87-0.9808.hdf5\n",
            "\n",
            "Epoch 88: saving model to ./data/model/all/88-0.9815.hdf5\n",
            "\n",
            "Epoch 89: saving model to ./data/model/all/89-0.9815.hdf5\n",
            "\n",
            "Epoch 90: saving model to ./data/model/all/90-0.9800.hdf5\n",
            "\n",
            "Epoch 91: saving model to ./data/model/all/91-0.9785.hdf5\n",
            "\n",
            "Epoch 92: saving model to ./data/model/all/92-0.9815.hdf5\n",
            "\n",
            "Epoch 93: saving model to ./data/model/all/93-0.9808.hdf5\n",
            "\n",
            "Epoch 94: saving model to ./data/model/all/94-0.9823.hdf5\n",
            "\n",
            "Epoch 95: saving model to ./data/model/all/95-0.9800.hdf5\n",
            "\n",
            "Epoch 96: saving model to ./data/model/all/96-0.9800.hdf5\n",
            "\n",
            "Epoch 97: saving model to ./data/model/all/97-0.9785.hdf5\n",
            "\n",
            "Epoch 98: saving model to ./data/model/all/98-0.9800.hdf5\n",
            "\n",
            "Epoch 99: saving model to ./data/model/all/99-0.9815.hdf5\n",
            "\n",
            "Epoch 100: saving model to ./data/model/all/100-0.9808.hdf5\n",
            "\n",
            "Epoch 101: saving model to ./data/model/all/101-0.9808.hdf5\n",
            "\n",
            "Epoch 102: saving model to ./data/model/all/102-0.9823.hdf5\n",
            "\n",
            "Epoch 103: saving model to ./data/model/all/103-0.9815.hdf5\n",
            "\n",
            "Epoch 104: saving model to ./data/model/all/104-0.9831.hdf5\n",
            "\n",
            "Epoch 105: saving model to ./data/model/all/105-0.9808.hdf5\n",
            "\n",
            "Epoch 106: saving model to ./data/model/all/106-0.9808.hdf5\n",
            "\n",
            "Epoch 107: saving model to ./data/model/all/107-0.9823.hdf5\n",
            "\n",
            "Epoch 108: saving model to ./data/model/all/108-0.9823.hdf5\n",
            "\n",
            "Epoch 109: saving model to ./data/model/all/109-0.9831.hdf5\n",
            "\n",
            "Epoch 110: saving model to ./data/model/all/110-0.9800.hdf5\n",
            "\n",
            "Epoch 111: saving model to ./data/model/all/111-0.9815.hdf5\n",
            "\n",
            "Epoch 112: saving model to ./data/model/all/112-0.9800.hdf5\n",
            "\n",
            "Epoch 113: saving model to ./data/model/all/113-0.9808.hdf5\n",
            "\n",
            "Epoch 114: saving model to ./data/model/all/114-0.9815.hdf5\n",
            "\n",
            "Epoch 115: saving model to ./data/model/all/115-0.9823.hdf5\n",
            "\n",
            "Epoch 116: saving model to ./data/model/all/116-0.9823.hdf5\n",
            "\n",
            "Epoch 117: saving model to ./data/model/all/117-0.9800.hdf5\n",
            "\n",
            "Epoch 118: saving model to ./data/model/all/118-0.9808.hdf5\n",
            "\n",
            "Epoch 119: saving model to ./data/model/all/119-0.9808.hdf5\n",
            "\n",
            "Epoch 120: saving model to ./data/model/all/120-0.9800.hdf5\n",
            "\n",
            "Epoch 121: saving model to ./data/model/all/121-0.9792.hdf5\n",
            "\n",
            "Epoch 122: saving model to ./data/model/all/122-0.9808.hdf5\n",
            "\n",
            "Epoch 123: saving model to ./data/model/all/123-0.9800.hdf5\n",
            "\n",
            "Epoch 124: saving model to ./data/model/all/124-0.9815.hdf5\n",
            "\n",
            "Epoch 125: saving model to ./data/model/all/125-0.9808.hdf5\n",
            "\n",
            "Epoch 126: saving model to ./data/model/all/126-0.9792.hdf5\n",
            "\n",
            "Epoch 127: saving model to ./data/model/all/127-0.9815.hdf5\n",
            "\n",
            "Epoch 128: saving model to ./data/model/all/128-0.9808.hdf5\n",
            "\n",
            "Epoch 129: saving model to ./data/model/all/129-0.9823.hdf5\n",
            "\n",
            "Epoch 130: saving model to ./data/model/all/130-0.9823.hdf5\n",
            "\n",
            "Epoch 131: saving model to ./data/model/all/131-0.9808.hdf5\n",
            "\n",
            "Epoch 132: saving model to ./data/model/all/132-0.9800.hdf5\n",
            "\n",
            "Epoch 133: saving model to ./data/model/all/133-0.9808.hdf5\n",
            "\n",
            "Epoch 134: saving model to ./data/model/all/134-0.9808.hdf5\n",
            "\n",
            "Epoch 135: saving model to ./data/model/all/135-0.9808.hdf5\n",
            "\n",
            "Epoch 136: saving model to ./data/model/all/136-0.9808.hdf5\n",
            "\n",
            "Epoch 137: saving model to ./data/model/all/137-0.9815.hdf5\n",
            "\n",
            "Epoch 138: saving model to ./data/model/all/138-0.9808.hdf5\n",
            "\n",
            "Epoch 139: saving model to ./data/model/all/139-0.9823.hdf5\n",
            "\n",
            "Epoch 140: saving model to ./data/model/all/140-0.9808.hdf5\n",
            "\n",
            "Epoch 141: saving model to ./data/model/all/141-0.9823.hdf5\n",
            "\n",
            "Epoch 142: saving model to ./data/model/all/142-0.9800.hdf5\n",
            "\n",
            "Epoch 143: saving model to ./data/model/all/143-0.9746.hdf5\n",
            "\n",
            "Epoch 144: saving model to ./data/model/all/144-0.9731.hdf5\n",
            "\n",
            "Epoch 145: saving model to ./data/model/all/145-0.9808.hdf5\n",
            "\n",
            "Epoch 146: saving model to ./data/model/all/146-0.9808.hdf5\n",
            "\n",
            "Epoch 147: saving model to ./data/model/all/147-0.9808.hdf5\n",
            "\n",
            "Epoch 148: saving model to ./data/model/all/148-0.9792.hdf5\n",
            "\n",
            "Epoch 149: saving model to ./data/model/all/149-0.9800.hdf5\n",
            "\n",
            "Epoch 150: saving model to ./data/model/all/150-0.9808.hdf5\n",
            "\n",
            "Epoch 151: saving model to ./data/model/all/151-0.9808.hdf5\n",
            "\n",
            "Epoch 152: saving model to ./data/model/all/152-0.9808.hdf5\n",
            "\n",
            "Epoch 153: saving model to ./data/model/all/153-0.9785.hdf5\n",
            "\n",
            "Epoch 154: saving model to ./data/model/all/154-0.9792.hdf5\n",
            "\n",
            "Epoch 155: saving model to ./data/model/all/155-0.9808.hdf5\n",
            "\n",
            "Epoch 156: saving model to ./data/model/all/156-0.9792.hdf5\n",
            "\n",
            "Epoch 157: saving model to ./data/model/all/157-0.9823.hdf5\n",
            "\n",
            "Epoch 158: saving model to ./data/model/all/158-0.9808.hdf5\n",
            "\n",
            "Epoch 159: saving model to ./data/model/all/159-0.9815.hdf5\n",
            "\n",
            "Epoch 160: saving model to ./data/model/all/160-0.9808.hdf5\n",
            "\n",
            "Epoch 161: saving model to ./data/model/all/161-0.9792.hdf5\n",
            "\n",
            "Epoch 162: saving model to ./data/model/all/162-0.9808.hdf5\n",
            "\n",
            "Epoch 163: saving model to ./data/model/all/163-0.9792.hdf5\n",
            "\n",
            "Epoch 164: saving model to ./data/model/all/164-0.9808.hdf5\n",
            "\n",
            "Epoch 165: saving model to ./data/model/all/165-0.9815.hdf5\n",
            "\n",
            "Epoch 166: saving model to ./data/model/all/166-0.9785.hdf5\n",
            "\n",
            "Epoch 167: saving model to ./data/model/all/167-0.9808.hdf5\n",
            "\n",
            "Epoch 168: saving model to ./data/model/all/168-0.9808.hdf5\n",
            "\n",
            "Epoch 169: saving model to ./data/model/all/169-0.9800.hdf5\n",
            "\n",
            "Epoch 170: saving model to ./data/model/all/170-0.9792.hdf5\n",
            "\n",
            "Epoch 171: saving model to ./data/model/all/171-0.9808.hdf5\n",
            "\n",
            "Epoch 172: saving model to ./data/model/all/172-0.9815.hdf5\n",
            "\n",
            "Epoch 173: saving model to ./data/model/all/173-0.9800.hdf5\n",
            "\n",
            "Epoch 174: saving model to ./data/model/all/174-0.9792.hdf5\n",
            "\n",
            "Epoch 175: saving model to ./data/model/all/175-0.9808.hdf5\n",
            "\n",
            "Epoch 176: saving model to ./data/model/all/176-0.9792.hdf5\n",
            "\n",
            "Epoch 177: saving model to ./data/model/all/177-0.9800.hdf5\n",
            "\n",
            "Epoch 178: saving model to ./data/model/all/178-0.9800.hdf5\n",
            "\n",
            "Epoch 179: saving model to ./data/model/all/179-0.9800.hdf5\n",
            "\n",
            "Epoch 180: saving model to ./data/model/all/180-0.9792.hdf5\n",
            "\n",
            "Epoch 181: saving model to ./data/model/all/181-0.9792.hdf5\n",
            "\n",
            "Epoch 182: saving model to ./data/model/all/182-0.9808.hdf5\n",
            "\n",
            "Epoch 183: saving model to ./data/model/all/183-0.9800.hdf5\n",
            "\n",
            "Epoch 184: saving model to ./data/model/all/184-0.9815.hdf5\n",
            "\n",
            "Epoch 185: saving model to ./data/model/all/185-0.9800.hdf5\n",
            "\n",
            "Epoch 186: saving model to ./data/model/all/186-0.9792.hdf5\n",
            "\n",
            "Epoch 187: saving model to ./data/model/all/187-0.9815.hdf5\n",
            "\n",
            "Epoch 188: saving model to ./data/model/all/188-0.9792.hdf5\n",
            "\n",
            "Epoch 189: saving model to ./data/model/all/189-0.9808.hdf5\n",
            "\n",
            "Epoch 190: saving model to ./data/model/all/190-0.9815.hdf5\n",
            "\n",
            "Epoch 191: saving model to ./data/model/all/191-0.9792.hdf5\n",
            "\n",
            "Epoch 192: saving model to ./data/model/all/192-0.9815.hdf5\n",
            "\n",
            "Epoch 193: saving model to ./data/model/all/193-0.9800.hdf5\n",
            "\n",
            "Epoch 194: saving model to ./data/model/all/194-0.9808.hdf5\n",
            "\n",
            "Epoch 195: saving model to ./data/model/all/195-0.9792.hdf5\n",
            "\n",
            "Epoch 196: saving model to ./data/model/all/196-0.9808.hdf5\n",
            "\n",
            "Epoch 197: saving model to ./data/model/all/197-0.9808.hdf5\n",
            "\n",
            "Epoch 198: saving model to ./data/model/all/198-0.9815.hdf5\n",
            "\n",
            "Epoch 199: saving model to ./data/model/all/199-0.9808.hdf5\n",
            "\n",
            "Epoch 200: saving model to ./data/model/all/200-0.9838.hdf5\n",
            "\n",
            "Epoch 201: saving model to ./data/model/all/201-0.9815.hdf5\n",
            "\n",
            "Epoch 202: saving model to ./data/model/all/202-0.9808.hdf5\n",
            "\n",
            "Epoch 203: saving model to ./data/model/all/203-0.9838.hdf5\n",
            "\n",
            "Epoch 204: saving model to ./data/model/all/204-0.9831.hdf5\n",
            "\n",
            "Epoch 205: saving model to ./data/model/all/205-0.9792.hdf5\n",
            "\n",
            "Epoch 206: saving model to ./data/model/all/206-0.9815.hdf5\n",
            "\n",
            "Epoch 207: saving model to ./data/model/all/207-0.9800.hdf5\n",
            "\n",
            "Epoch 208: saving model to ./data/model/all/208-0.9831.hdf5\n",
            "\n",
            "Epoch 209: saving model to ./data/model/all/209-0.9838.hdf5\n",
            "\n",
            "Epoch 210: saving model to ./data/model/all/210-0.9792.hdf5\n",
            "\n",
            "Epoch 211: saving model to ./data/model/all/211-0.9808.hdf5\n",
            "\n",
            "Epoch 212: saving model to ./data/model/all/212-0.9792.hdf5\n",
            "\n",
            "Epoch 213: saving model to ./data/model/all/213-0.9823.hdf5\n",
            "\n",
            "Epoch 214: saving model to ./data/model/all/214-0.9808.hdf5\n",
            "\n",
            "Epoch 215: saving model to ./data/model/all/215-0.9792.hdf5\n",
            "\n",
            "Epoch 216: saving model to ./data/model/all/216-0.9800.hdf5\n",
            "\n",
            "Epoch 217: saving model to ./data/model/all/217-0.9792.hdf5\n",
            "\n",
            "Epoch 218: saving model to ./data/model/all/218-0.9815.hdf5\n",
            "\n",
            "Epoch 219: saving model to ./data/model/all/219-0.9800.hdf5\n",
            "\n",
            "Epoch 220: saving model to ./data/model/all/220-0.9808.hdf5\n",
            "\n",
            "Epoch 221: saving model to ./data/model/all/221-0.9815.hdf5\n",
            "\n",
            "Epoch 222: saving model to ./data/model/all/222-0.9785.hdf5\n",
            "\n",
            "Epoch 223: saving model to ./data/model/all/223-0.9823.hdf5\n",
            "\n",
            "Epoch 224: saving model to ./data/model/all/224-0.9838.hdf5\n",
            "\n",
            "Epoch 225: saving model to ./data/model/all/225-0.9815.hdf5\n",
            "\n",
            "Epoch 226: saving model to ./data/model/all/226-0.9808.hdf5\n",
            "\n",
            "Epoch 227: saving model to ./data/model/all/227-0.9831.hdf5\n",
            "\n",
            "Epoch 228: saving model to ./data/model/all/228-0.9800.hdf5\n",
            "\n",
            "Epoch 229: saving model to ./data/model/all/229-0.9815.hdf5\n",
            "\n",
            "Epoch 230: saving model to ./data/model/all/230-0.9808.hdf5\n",
            "\n",
            "Epoch 231: saving model to ./data/model/all/231-0.9800.hdf5\n",
            "\n",
            "Epoch 232: saving model to ./data/model/all/232-0.9800.hdf5\n",
            "\n",
            "Epoch 233: saving model to ./data/model/all/233-0.9808.hdf5\n",
            "\n",
            "Epoch 234: saving model to ./data/model/all/234-0.9815.hdf5\n",
            "\n",
            "Epoch 235: saving model to ./data/model/all/235-0.9831.hdf5\n",
            "\n",
            "Epoch 236: saving model to ./data/model/all/236-0.9815.hdf5\n",
            "\n",
            "Epoch 237: saving model to ./data/model/all/237-0.9815.hdf5\n",
            "\n",
            "Epoch 238: saving model to ./data/model/all/238-0.9815.hdf5\n",
            "\n",
            "Epoch 239: saving model to ./data/model/all/239-0.9792.hdf5\n",
            "\n",
            "Epoch 240: saving model to ./data/model/all/240-0.9808.hdf5\n",
            "\n",
            "Epoch 241: saving model to ./data/model/all/241-0.9808.hdf5\n",
            "\n",
            "Epoch 242: saving model to ./data/model/all/242-0.9800.hdf5\n",
            "\n",
            "Epoch 243: saving model to ./data/model/all/243-0.9808.hdf5\n",
            "\n",
            "Epoch 244: saving model to ./data/model/all/244-0.9800.hdf5\n",
            "\n",
            "Epoch 245: saving model to ./data/model/all/245-0.9800.hdf5\n",
            "\n",
            "Epoch 246: saving model to ./data/model/all/246-0.9823.hdf5\n",
            "\n",
            "Epoch 247: saving model to ./data/model/all/247-0.9815.hdf5\n",
            "\n",
            "Epoch 248: saving model to ./data/model/all/248-0.9800.hdf5\n",
            "\n",
            "Epoch 249: saving model to ./data/model/all/249-0.9815.hdf5\n",
            "\n",
            "Epoch 250: saving model to ./data/model/all/250-0.9808.hdf5\n",
            "\n",
            "Epoch 251: saving model to ./data/model/all/251-0.9815.hdf5\n",
            "\n",
            "Epoch 252: saving model to ./data/model/all/252-0.9800.hdf5\n",
            "\n",
            "Epoch 253: saving model to ./data/model/all/253-0.9823.hdf5\n",
            "\n",
            "Epoch 254: saving model to ./data/model/all/254-0.9823.hdf5\n",
            "\n",
            "Epoch 255: saving model to ./data/model/all/255-0.9823.hdf5\n",
            "\n",
            "Epoch 256: saving model to ./data/model/all/256-0.9815.hdf5\n",
            "\n",
            "Epoch 257: saving model to ./data/model/all/257-0.9815.hdf5\n",
            "\n",
            "Epoch 258: saving model to ./data/model/all/258-0.9808.hdf5\n",
            "\n",
            "Epoch 259: saving model to ./data/model/all/259-0.9823.hdf5\n",
            "\n",
            "Epoch 260: saving model to ./data/model/all/260-0.9823.hdf5\n",
            "\n",
            "Epoch 261: saving model to ./data/model/all/261-0.9815.hdf5\n",
            "\n",
            "Epoch 262: saving model to ./data/model/all/262-0.9823.hdf5\n",
            "\n",
            "Epoch 263: saving model to ./data/model/all/263-0.9808.hdf5\n",
            "\n",
            "Epoch 264: saving model to ./data/model/all/264-0.9808.hdf5\n",
            "\n",
            "Epoch 265: saving model to ./data/model/all/265-0.9815.hdf5\n",
            "\n",
            "Epoch 266: saving model to ./data/model/all/266-0.9823.hdf5\n",
            "\n",
            "Epoch 267: saving model to ./data/model/all/267-0.9823.hdf5\n",
            "\n",
            "Epoch 268: saving model to ./data/model/all/268-0.9823.hdf5\n",
            "\n",
            "Epoch 269: saving model to ./data/model/all/269-0.9815.hdf5\n",
            "\n",
            "Epoch 270: saving model to ./data/model/all/270-0.9823.hdf5\n",
            "\n",
            "Epoch 271: saving model to ./data/model/all/271-0.9815.hdf5\n",
            "\n",
            "Epoch 272: saving model to ./data/model/all/272-0.9808.hdf5\n",
            "\n",
            "Epoch 273: saving model to ./data/model/all/273-0.9823.hdf5\n",
            "\n",
            "Epoch 274: saving model to ./data/model/all/274-0.9815.hdf5\n",
            "\n",
            "Epoch 275: saving model to ./data/model/all/275-0.9777.hdf5\n",
            "\n",
            "Epoch 276: saving model to ./data/model/all/276-0.9777.hdf5\n",
            "\n",
            "Epoch 277: saving model to ./data/model/all/277-0.9815.hdf5\n",
            "\n",
            "Epoch 278: saving model to ./data/model/all/278-0.9823.hdf5\n",
            "\n",
            "Epoch 279: saving model to ./data/model/all/279-0.9769.hdf5\n",
            "\n",
            "Epoch 280: saving model to ./data/model/all/280-0.9815.hdf5\n",
            "\n",
            "Epoch 281: saving model to ./data/model/all/281-0.9808.hdf5\n",
            "\n",
            "Epoch 282: saving model to ./data/model/all/282-0.9808.hdf5\n",
            "\n",
            "Epoch 283: saving model to ./data/model/all/283-0.9815.hdf5\n",
            "\n",
            "Epoch 284: saving model to ./data/model/all/284-0.9815.hdf5\n",
            "\n",
            "Epoch 285: saving model to ./data/model/all/285-0.9823.hdf5\n",
            "\n",
            "Epoch 286: saving model to ./data/model/all/286-0.9815.hdf5\n",
            "\n",
            "Epoch 287: saving model to ./data/model/all/287-0.9808.hdf5\n",
            "\n",
            "Epoch 288: saving model to ./data/model/all/288-0.9808.hdf5\n",
            "\n",
            "Epoch 289: saving model to ./data/model/all/289-0.9815.hdf5\n",
            "\n",
            "Epoch 290: saving model to ./data/model/all/290-0.9823.hdf5\n",
            "\n",
            "Epoch 291: saving model to ./data/model/all/291-0.9808.hdf5\n",
            "\n",
            "Epoch 292: saving model to ./data/model/all/292-0.9815.hdf5\n",
            "\n",
            "Epoch 293: saving model to ./data/model/all/293-0.9838.hdf5\n",
            "\n",
            "Epoch 294: saving model to ./data/model/all/294-0.9823.hdf5\n",
            "\n",
            "Epoch 295: saving model to ./data/model/all/295-0.9823.hdf5\n",
            "\n",
            "Epoch 296: saving model to ./data/model/all/296-0.9823.hdf5\n",
            "\n",
            "Epoch 297: saving model to ./data/model/all/297-0.9808.hdf5\n",
            "\n",
            "Epoch 298: saving model to ./data/model/all/298-0.9815.hdf5\n",
            "\n",
            "Epoch 299: saving model to ./data/model/all/299-0.9831.hdf5\n",
            "\n",
            "Epoch 300: saving model to ./data/model/all/300-0.9831.hdf5\n",
            "\n",
            "Epoch 301: saving model to ./data/model/all/301-0.9785.hdf5\n",
            "\n",
            "Epoch 302: saving model to ./data/model/all/302-0.9823.hdf5\n",
            "\n",
            "Epoch 303: saving model to ./data/model/all/303-0.9815.hdf5\n",
            "\n",
            "Epoch 304: saving model to ./data/model/all/304-0.9815.hdf5\n",
            "\n",
            "Epoch 305: saving model to ./data/model/all/305-0.9823.hdf5\n",
            "\n",
            "Epoch 306: saving model to ./data/model/all/306-0.9815.hdf5\n",
            "\n",
            "Epoch 307: saving model to ./data/model/all/307-0.9808.hdf5\n",
            "\n",
            "Epoch 308: saving model to ./data/model/all/308-0.9823.hdf5\n",
            "\n",
            "Epoch 309: saving model to ./data/model/all/309-0.9815.hdf5\n",
            "\n",
            "Epoch 310: saving model to ./data/model/all/310-0.9808.hdf5\n",
            "\n",
            "Epoch 311: saving model to ./data/model/all/311-0.9815.hdf5\n",
            "\n",
            "Epoch 312: saving model to ./data/model/all/312-0.9808.hdf5\n",
            "\n",
            "Epoch 313: saving model to ./data/model/all/313-0.9815.hdf5\n",
            "\n",
            "Epoch 314: saving model to ./data/model/all/314-0.9823.hdf5\n",
            "\n",
            "Epoch 315: saving model to ./data/model/all/315-0.9823.hdf5\n",
            "\n",
            "Epoch 316: saving model to ./data/model/all/316-0.9823.hdf5\n",
            "\n",
            "Epoch 317: saving model to ./data/model/all/317-0.9823.hdf5\n",
            "\n",
            "Epoch 318: saving model to ./data/model/all/318-0.9823.hdf5\n",
            "\n",
            "Epoch 319: saving model to ./data/model/all/319-0.9838.hdf5\n",
            "\n",
            "Epoch 320: saving model to ./data/model/all/320-0.9838.hdf5\n",
            "\n",
            "Epoch 321: saving model to ./data/model/all/321-0.9815.hdf5\n",
            "\n",
            "Epoch 322: saving model to ./data/model/all/322-0.9823.hdf5\n",
            "\n",
            "Epoch 323: saving model to ./data/model/all/323-0.9815.hdf5\n",
            "\n",
            "Epoch 324: saving model to ./data/model/all/324-0.9823.hdf5\n",
            "\n",
            "Epoch 325: saving model to ./data/model/all/325-0.9823.hdf5\n",
            "\n",
            "Epoch 326: saving model to ./data/model/all/326-0.9831.hdf5\n",
            "\n",
            "Epoch 327: saving model to ./data/model/all/327-0.9815.hdf5\n",
            "\n",
            "Epoch 328: saving model to ./data/model/all/328-0.9808.hdf5\n",
            "\n",
            "Epoch 329: saving model to ./data/model/all/329-0.9831.hdf5\n",
            "\n",
            "Epoch 330: saving model to ./data/model/all/330-0.9831.hdf5\n",
            "\n",
            "Epoch 331: saving model to ./data/model/all/331-0.9792.hdf5\n",
            "\n",
            "Epoch 332: saving model to ./data/model/all/332-0.9815.hdf5\n",
            "\n",
            "Epoch 333: saving model to ./data/model/all/333-0.9815.hdf5\n",
            "\n",
            "Epoch 334: saving model to ./data/model/all/334-0.9831.hdf5\n",
            "\n",
            "Epoch 335: saving model to ./data/model/all/335-0.9800.hdf5\n",
            "\n",
            "Epoch 336: saving model to ./data/model/all/336-0.9815.hdf5\n",
            "\n",
            "Epoch 337: saving model to ./data/model/all/337-0.9815.hdf5\n",
            "\n",
            "Epoch 338: saving model to ./data/model/all/338-0.9823.hdf5\n",
            "\n",
            "Epoch 339: saving model to ./data/model/all/339-0.9823.hdf5\n",
            "\n",
            "Epoch 340: saving model to ./data/model/all/340-0.9815.hdf5\n",
            "\n",
            "Epoch 341: saving model to ./data/model/all/341-0.9823.hdf5\n",
            "\n",
            "Epoch 342: saving model to ./data/model/all/342-0.9823.hdf5\n",
            "\n",
            "Epoch 343: saving model to ./data/model/all/343-0.9823.hdf5\n",
            "\n",
            "Epoch 344: saving model to ./data/model/all/344-0.9815.hdf5\n",
            "\n",
            "Epoch 345: saving model to ./data/model/all/345-0.9823.hdf5\n",
            "\n",
            "Epoch 346: saving model to ./data/model/all/346-0.9815.hdf5\n",
            "\n",
            "Epoch 347: saving model to ./data/model/all/347-0.9815.hdf5\n",
            "\n",
            "Epoch 348: saving model to ./data/model/all/348-0.9815.hdf5\n",
            "\n",
            "Epoch 349: saving model to ./data/model/all/349-0.9831.hdf5\n",
            "\n",
            "Epoch 350: saving model to ./data/model/all/350-0.9800.hdf5\n",
            "\n",
            "Epoch 351: saving model to ./data/model/all/351-0.9823.hdf5\n",
            "\n",
            "Epoch 352: saving model to ./data/model/all/352-0.9815.hdf5\n",
            "\n",
            "Epoch 353: saving model to ./data/model/all/353-0.9823.hdf5\n",
            "\n",
            "Epoch 354: saving model to ./data/model/all/354-0.9808.hdf5\n",
            "\n",
            "Epoch 355: saving model to ./data/model/all/355-0.9815.hdf5\n",
            "\n",
            "Epoch 356: saving model to ./data/model/all/356-0.9808.hdf5\n",
            "\n",
            "Epoch 357: saving model to ./data/model/all/357-0.9838.hdf5\n",
            "\n",
            "Epoch 358: saving model to ./data/model/all/358-0.9823.hdf5\n",
            "\n",
            "Epoch 359: saving model to ./data/model/all/359-0.9823.hdf5\n",
            "\n",
            "Epoch 360: saving model to ./data/model/all/360-0.9823.hdf5\n",
            "\n",
            "Epoch 361: saving model to ./data/model/all/361-0.9808.hdf5\n",
            "\n",
            "Epoch 362: saving model to ./data/model/all/362-0.9831.hdf5\n",
            "\n",
            "Epoch 363: saving model to ./data/model/all/363-0.9831.hdf5\n",
            "\n",
            "Epoch 364: saving model to ./data/model/all/364-0.9831.hdf5\n",
            "\n",
            "Epoch 365: saving model to ./data/model/all/365-0.9815.hdf5\n",
            "\n",
            "Epoch 366: saving model to ./data/model/all/366-0.9823.hdf5\n",
            "\n",
            "Epoch 367: saving model to ./data/model/all/367-0.9823.hdf5\n",
            "\n",
            "Epoch 368: saving model to ./data/model/all/368-0.9808.hdf5\n",
            "\n",
            "Epoch 369: saving model to ./data/model/all/369-0.9815.hdf5\n",
            "\n",
            "Epoch 370: saving model to ./data/model/all/370-0.9823.hdf5\n",
            "\n",
            "Epoch 371: saving model to ./data/model/all/371-0.9823.hdf5\n",
            "\n",
            "Epoch 372: saving model to ./data/model/all/372-0.9815.hdf5\n",
            "\n",
            "Epoch 373: saving model to ./data/model/all/373-0.9838.hdf5\n",
            "\n",
            "Epoch 374: saving model to ./data/model/all/374-0.9838.hdf5\n",
            "\n",
            "Epoch 375: saving model to ./data/model/all/375-0.9823.hdf5\n",
            "\n",
            "Epoch 376: saving model to ./data/model/all/376-0.9823.hdf5\n",
            "\n",
            "Epoch 377: saving model to ./data/model/all/377-0.9815.hdf5\n",
            "\n",
            "Epoch 378: saving model to ./data/model/all/378-0.9831.hdf5\n",
            "\n",
            "Epoch 379: saving model to ./data/model/all/379-0.9823.hdf5\n",
            "\n",
            "Epoch 380: saving model to ./data/model/all/380-0.9815.hdf5\n",
            "\n",
            "Epoch 381: saving model to ./data/model/all/381-0.9823.hdf5\n",
            "\n",
            "Epoch 382: saving model to ./data/model/all/382-0.9815.hdf5\n",
            "\n",
            "Epoch 383: saving model to ./data/model/all/383-0.9831.hdf5\n",
            "\n",
            "Epoch 384: saving model to ./data/model/all/384-0.9815.hdf5\n",
            "\n",
            "Epoch 385: saving model to ./data/model/all/385-0.9769.hdf5\n",
            "\n",
            "Epoch 386: saving model to ./data/model/all/386-0.9823.hdf5\n",
            "\n",
            "Epoch 387: saving model to ./data/model/all/387-0.9815.hdf5\n",
            "\n",
            "Epoch 388: saving model to ./data/model/all/388-0.9823.hdf5\n",
            "\n",
            "Epoch 389: saving model to ./data/model/all/389-0.9823.hdf5\n",
            "\n",
            "Epoch 390: saving model to ./data/model/all/390-0.9823.hdf5\n",
            "\n",
            "Epoch 391: saving model to ./data/model/all/391-0.9831.hdf5\n",
            "\n",
            "Epoch 392: saving model to ./data/model/all/392-0.9746.hdf5\n",
            "\n",
            "Epoch 393: saving model to ./data/model/all/393-0.9815.hdf5\n",
            "\n",
            "Epoch 394: saving model to ./data/model/all/394-0.9831.hdf5\n",
            "\n",
            "Epoch 395: saving model to ./data/model/all/395-0.9808.hdf5\n",
            "\n",
            "Epoch 396: saving model to ./data/model/all/396-0.9838.hdf5\n",
            "\n",
            "Epoch 397: saving model to ./data/model/all/397-0.9838.hdf5\n",
            "\n",
            "Epoch 398: saving model to ./data/model/all/398-0.9815.hdf5\n",
            "\n",
            "Epoch 399: saving model to ./data/model/all/399-0.9823.hdf5\n",
            "\n",
            "Epoch 400: saving model to ./data/model/all/400-0.9823.hdf5\n",
            "\n",
            "Epoch 401: saving model to ./data/model/all/401-0.9815.hdf5\n",
            "\n",
            "Epoch 402: saving model to ./data/model/all/402-0.9831.hdf5\n",
            "\n",
            "Epoch 403: saving model to ./data/model/all/403-0.9823.hdf5\n",
            "\n",
            "Epoch 404: saving model to ./data/model/all/404-0.9800.hdf5\n",
            "\n",
            "Epoch 405: saving model to ./data/model/all/405-0.9823.hdf5\n",
            "\n",
            "Epoch 406: saving model to ./data/model/all/406-0.9823.hdf5\n",
            "\n",
            "Epoch 407: saving model to ./data/model/all/407-0.9823.hdf5\n",
            "\n",
            "Epoch 408: saving model to ./data/model/all/408-0.9823.hdf5\n",
            "\n",
            "Epoch 409: saving model to ./data/model/all/409-0.9823.hdf5\n",
            "\n",
            "Epoch 410: saving model to ./data/model/all/410-0.9823.hdf5\n",
            "\n",
            "Epoch 411: saving model to ./data/model/all/411-0.9838.hdf5\n",
            "\n",
            "Epoch 412: saving model to ./data/model/all/412-0.9815.hdf5\n",
            "\n",
            "Epoch 413: saving model to ./data/model/all/413-0.9823.hdf5\n",
            "\n",
            "Epoch 414: saving model to ./data/model/all/414-0.9762.hdf5\n",
            "\n",
            "Epoch 415: saving model to ./data/model/all/415-0.9846.hdf5\n",
            "\n",
            "Epoch 416: saving model to ./data/model/all/416-0.9831.hdf5\n",
            "\n",
            "Epoch 417: saving model to ./data/model/all/417-0.9823.hdf5\n",
            "\n",
            "Epoch 418: saving model to ./data/model/all/418-0.9831.hdf5\n",
            "\n",
            "Epoch 419: saving model to ./data/model/all/419-0.9831.hdf5\n",
            "\n",
            "Epoch 420: saving model to ./data/model/all/420-0.9838.hdf5\n",
            "\n",
            "Epoch 421: saving model to ./data/model/all/421-0.9823.hdf5\n",
            "\n",
            "Epoch 422: saving model to ./data/model/all/422-0.9823.hdf5\n",
            "\n",
            "Epoch 423: saving model to ./data/model/all/423-0.9823.hdf5\n",
            "\n",
            "Epoch 424: saving model to ./data/model/all/424-0.9823.hdf5\n",
            "\n",
            "Epoch 425: saving model to ./data/model/all/425-0.9823.hdf5\n",
            "\n",
            "Epoch 426: saving model to ./data/model/all/426-0.9815.hdf5\n",
            "\n",
            "Epoch 427: saving model to ./data/model/all/427-0.9823.hdf5\n",
            "\n",
            "Epoch 428: saving model to ./data/model/all/428-0.9823.hdf5\n",
            "\n",
            "Epoch 429: saving model to ./data/model/all/429-0.9823.hdf5\n",
            "\n",
            "Epoch 430: saving model to ./data/model/all/430-0.9815.hdf5\n",
            "\n",
            "Epoch 431: saving model to ./data/model/all/431-0.9815.hdf5\n",
            "\n",
            "Epoch 432: saving model to ./data/model/all/432-0.9823.hdf5\n",
            "\n",
            "Epoch 433: saving model to ./data/model/all/433-0.9823.hdf5\n",
            "\n",
            "Epoch 434: saving model to ./data/model/all/434-0.9823.hdf5\n",
            "\n",
            "Epoch 435: saving model to ./data/model/all/435-0.9831.hdf5\n",
            "\n",
            "Epoch 436: saving model to ./data/model/all/436-0.9823.hdf5\n",
            "\n",
            "Epoch 437: saving model to ./data/model/all/437-0.9823.hdf5\n",
            "\n",
            "Epoch 438: saving model to ./data/model/all/438-0.9854.hdf5\n",
            "\n",
            "Epoch 439: saving model to ./data/model/all/439-0.9815.hdf5\n",
            "\n",
            "Epoch 440: saving model to ./data/model/all/440-0.9823.hdf5\n",
            "\n",
            "Epoch 441: saving model to ./data/model/all/441-0.9815.hdf5\n",
            "\n",
            "Epoch 442: saving model to ./data/model/all/442-0.9823.hdf5\n",
            "\n",
            "Epoch 443: saving model to ./data/model/all/443-0.9815.hdf5\n",
            "\n",
            "Epoch 444: saving model to ./data/model/all/444-0.9831.hdf5\n",
            "\n",
            "Epoch 445: saving model to ./data/model/all/445-0.9838.hdf5\n",
            "\n",
            "Epoch 446: saving model to ./data/model/all/446-0.9823.hdf5\n",
            "\n",
            "Epoch 447: saving model to ./data/model/all/447-0.9823.hdf5\n",
            "\n",
            "Epoch 448: saving model to ./data/model/all/448-0.9800.hdf5\n",
            "\n",
            "Epoch 449: saving model to ./data/model/all/449-0.9846.hdf5\n",
            "\n",
            "Epoch 450: saving model to ./data/model/all/450-0.9823.hdf5\n",
            "\n",
            "Epoch 451: saving model to ./data/model/all/451-0.9815.hdf5\n",
            "\n",
            "Epoch 452: saving model to ./data/model/all/452-0.9823.hdf5\n",
            "\n",
            "Epoch 453: saving model to ./data/model/all/453-0.9823.hdf5\n",
            "\n",
            "Epoch 454: saving model to ./data/model/all/454-0.9831.hdf5\n",
            "\n",
            "Epoch 455: saving model to ./data/model/all/455-0.9831.hdf5\n",
            "\n",
            "Epoch 456: saving model to ./data/model/all/456-0.9831.hdf5\n",
            "\n",
            "Epoch 457: saving model to ./data/model/all/457-0.9823.hdf5\n",
            "\n",
            "Epoch 458: saving model to ./data/model/all/458-0.9823.hdf5\n",
            "\n",
            "Epoch 459: saving model to ./data/model/all/459-0.9815.hdf5\n",
            "\n",
            "Epoch 460: saving model to ./data/model/all/460-0.9823.hdf5\n",
            "\n",
            "Epoch 461: saving model to ./data/model/all/461-0.9831.hdf5\n",
            "\n",
            "Epoch 462: saving model to ./data/model/all/462-0.9815.hdf5\n",
            "\n",
            "Epoch 463: saving model to ./data/model/all/463-0.9823.hdf5\n",
            "\n",
            "Epoch 464: saving model to ./data/model/all/464-0.9823.hdf5\n",
            "\n",
            "Epoch 465: saving model to ./data/model/all/465-0.9831.hdf5\n",
            "\n",
            "Epoch 466: saving model to ./data/model/all/466-0.9831.hdf5\n",
            "\n",
            "Epoch 467: saving model to ./data/model/all/467-0.9815.hdf5\n",
            "\n",
            "Epoch 468: saving model to ./data/model/all/468-0.9831.hdf5\n",
            "\n",
            "Epoch 469: saving model to ./data/model/all/469-0.9823.hdf5\n",
            "\n",
            "Epoch 470: saving model to ./data/model/all/470-0.9831.hdf5\n",
            "\n",
            "Epoch 471: saving model to ./data/model/all/471-0.9831.hdf5\n",
            "\n",
            "Epoch 472: saving model to ./data/model/all/472-0.9823.hdf5\n",
            "\n",
            "Epoch 473: saving model to ./data/model/all/473-0.9823.hdf5\n",
            "\n",
            "Epoch 474: saving model to ./data/model/all/474-0.9838.hdf5\n",
            "\n",
            "Epoch 475: saving model to ./data/model/all/475-0.9831.hdf5\n",
            "\n",
            "Epoch 476: saving model to ./data/model/all/476-0.9838.hdf5\n",
            "\n",
            "Epoch 477: saving model to ./data/model/all/477-0.9831.hdf5\n",
            "\n",
            "Epoch 478: saving model to ./data/model/all/478-0.9808.hdf5\n",
            "\n",
            "Epoch 479: saving model to ./data/model/all/479-0.9808.hdf5\n",
            "\n",
            "Epoch 480: saving model to ./data/model/all/480-0.9831.hdf5\n",
            "\n",
            "Epoch 481: saving model to ./data/model/all/481-0.9831.hdf5\n",
            "\n",
            "Epoch 482: saving model to ./data/model/all/482-0.9838.hdf5\n",
            "\n",
            "Epoch 483: saving model to ./data/model/all/483-0.9823.hdf5\n",
            "\n",
            "Epoch 484: saving model to ./data/model/all/484-0.9831.hdf5\n",
            "\n",
            "Epoch 485: saving model to ./data/model/all/485-0.9823.hdf5\n",
            "\n",
            "Epoch 486: saving model to ./data/model/all/486-0.9831.hdf5\n",
            "\n",
            "Epoch 487: saving model to ./data/model/all/487-0.9823.hdf5\n",
            "\n",
            "Epoch 488: saving model to ./data/model/all/488-0.9823.hdf5\n",
            "\n",
            "Epoch 489: saving model to ./data/model/all/489-0.9823.hdf5\n",
            "\n",
            "Epoch 490: saving model to ./data/model/all/490-0.9838.hdf5\n",
            "\n",
            "Epoch 491: saving model to ./data/model/all/491-0.9838.hdf5\n",
            "\n",
            "Epoch 492: saving model to ./data/model/all/492-0.9831.hdf5\n",
            "\n",
            "Epoch 493: saving model to ./data/model/all/493-0.9838.hdf5\n",
            "\n",
            "Epoch 494: saving model to ./data/model/all/494-0.9823.hdf5\n",
            "\n",
            "Epoch 495: saving model to ./data/model/all/495-0.9831.hdf5\n",
            "\n",
            "Epoch 496: saving model to ./data/model/all/496-0.9831.hdf5\n",
            "\n",
            "Epoch 497: saving model to ./data/model/all/497-0.9831.hdf5\n",
            "\n",
            "Epoch 498: saving model to ./data/model/all/498-0.9831.hdf5\n",
            "\n",
            "Epoch 499: saving model to ./data/model/all/499-0.9823.hdf5\n",
            "\n",
            "Epoch 500: saving model to ./data/model/all/500-0.9823.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 결과를 출력합니다.\n",
        "score=model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRj71FAcWZw2",
        "outputId": "19ef8c56-2da3-48eb-d709-ed44fd6e026c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9462\n",
            "Test accuracy: 0.9461538195610046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프 확인을 위한 긴 학습\n",
        "history=model.fit(X_train, y_train, epochs=500, batch_size=500, validation_split=0.25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVGbUfjaWcyk",
        "outputId": "b31977ad-51f4-4251-9300-19f4174f088f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.0395 - accuracy: 0.9882 - val_loss: 0.0576 - val_accuracy: 0.9823\n",
            "Epoch 2/500\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0405 - accuracy: 0.9882 - val_loss: 0.0617 - val_accuracy: 0.9808\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0440 - accuracy: 0.9877 - val_loss: 0.0608 - val_accuracy: 0.9838\n",
            "Epoch 4/500\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0438 - accuracy: 0.9877 - val_loss: 0.0686 - val_accuracy: 0.9823\n",
            "Epoch 5/500\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.0419 - accuracy: 0.9867 - val_loss: 0.0649 - val_accuracy: 0.9831\n",
            "Epoch 6/500\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0383 - accuracy: 0.9882 - val_loss: 0.0603 - val_accuracy: 0.9831\n",
            "Epoch 7/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0393 - accuracy: 0.9892 - val_loss: 0.0587 - val_accuracy: 0.9846\n",
            "Epoch 8/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0376 - accuracy: 0.9895 - val_loss: 0.0568 - val_accuracy: 0.9831\n",
            "Epoch 9/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0376 - accuracy: 0.9890 - val_loss: 0.0626 - val_accuracy: 0.9823\n",
            "Epoch 10/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0368 - accuracy: 0.9892 - val_loss: 0.0583 - val_accuracy: 0.9838\n",
            "Epoch 11/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0420 - accuracy: 0.9864 - val_loss: 0.0596 - val_accuracy: 0.9831\n",
            "Epoch 12/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0419 - accuracy: 0.9869 - val_loss: 0.0589 - val_accuracy: 0.9823\n",
            "Epoch 13/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0390 - accuracy: 0.9895 - val_loss: 0.0614 - val_accuracy: 0.9823\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0392 - accuracy: 0.9900 - val_loss: 0.0649 - val_accuracy: 0.9831\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 0.9885 - val_loss: 0.0665 - val_accuracy: 0.9831\n",
            "Epoch 16/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0427 - accuracy: 0.9874 - val_loss: 0.0628 - val_accuracy: 0.9823\n",
            "Epoch 17/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0415 - accuracy: 0.9887 - val_loss: 0.0582 - val_accuracy: 0.9838\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0427 - accuracy: 0.9879 - val_loss: 0.0582 - val_accuracy: 0.9823\n",
            "Epoch 19/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0416 - accuracy: 0.9874 - val_loss: 0.0588 - val_accuracy: 0.9823\n",
            "Epoch 20/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0393 - accuracy: 0.9892 - val_loss: 0.0555 - val_accuracy: 0.9838\n",
            "Epoch 21/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0389 - accuracy: 0.9885 - val_loss: 0.0607 - val_accuracy: 0.9823\n",
            "Epoch 22/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0431 - accuracy: 0.9874 - val_loss: 0.0599 - val_accuracy: 0.9823\n",
            "Epoch 23/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0437 - accuracy: 0.9861 - val_loss: 0.0587 - val_accuracy: 0.9831\n",
            "Epoch 24/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9877 - val_loss: 0.0628 - val_accuracy: 0.9815\n",
            "Epoch 25/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0402 - accuracy: 0.9890 - val_loss: 0.0602 - val_accuracy: 0.9846\n",
            "Epoch 26/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0370 - accuracy: 0.9902 - val_loss: 0.0611 - val_accuracy: 0.9823\n",
            "Epoch 27/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 0.9890 - val_loss: 0.0578 - val_accuracy: 0.9838\n",
            "Epoch 28/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 0.9895 - val_loss: 0.0636 - val_accuracy: 0.9823\n",
            "Epoch 29/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0374 - accuracy: 0.9902 - val_loss: 0.0600 - val_accuracy: 0.9846\n",
            "Epoch 30/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0396 - accuracy: 0.9882 - val_loss: 0.0619 - val_accuracy: 0.9831\n",
            "Epoch 31/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9890 - val_loss: 0.0617 - val_accuracy: 0.9831\n",
            "Epoch 32/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0368 - accuracy: 0.9895 - val_loss: 0.0587 - val_accuracy: 0.9823\n",
            "Epoch 33/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0386 - accuracy: 0.9887 - val_loss: 0.0613 - val_accuracy: 0.9838\n",
            "Epoch 34/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0391 - accuracy: 0.9890 - val_loss: 0.0583 - val_accuracy: 0.9831\n",
            "Epoch 35/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0380 - accuracy: 0.9890 - val_loss: 0.0587 - val_accuracy: 0.9838\n",
            "Epoch 36/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 0.9902 - val_loss: 0.0608 - val_accuracy: 0.9823\n",
            "Epoch 37/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0365 - accuracy: 0.9897 - val_loss: 0.0597 - val_accuracy: 0.9838\n",
            "Epoch 38/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.9895 - val_loss: 0.0590 - val_accuracy: 0.9831\n",
            "Epoch 39/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0375 - accuracy: 0.9897 - val_loss: 0.0608 - val_accuracy: 0.9831\n",
            "Epoch 40/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0388 - accuracy: 0.9882 - val_loss: 0.0612 - val_accuracy: 0.9831\n",
            "Epoch 41/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0435 - accuracy: 0.9867 - val_loss: 0.0687 - val_accuracy: 0.9831\n",
            "Epoch 42/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0383 - accuracy: 0.9885 - val_loss: 0.0674 - val_accuracy: 0.9831\n",
            "Epoch 43/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0386 - accuracy: 0.9882 - val_loss: 0.0656 - val_accuracy: 0.9831\n",
            "Epoch 44/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0388 - accuracy: 0.9879 - val_loss: 0.0644 - val_accuracy: 0.9831\n",
            "Epoch 45/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0390 - accuracy: 0.9892 - val_loss: 0.0602 - val_accuracy: 0.9838\n",
            "Epoch 46/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0381 - accuracy: 0.9890 - val_loss: 0.0591 - val_accuracy: 0.9831\n",
            "Epoch 47/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0400 - accuracy: 0.9879 - val_loss: 0.0574 - val_accuracy: 0.9831\n",
            "Epoch 48/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9910 - val_loss: 0.0618 - val_accuracy: 0.9815\n",
            "Epoch 49/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0374 - accuracy: 0.9897 - val_loss: 0.0608 - val_accuracy: 0.9831\n",
            "Epoch 50/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0368 - accuracy: 0.9890 - val_loss: 0.0591 - val_accuracy: 0.9854\n",
            "Epoch 51/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 0.0596 - val_accuracy: 0.9846\n",
            "Epoch 52/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0362 - accuracy: 0.9902 - val_loss: 0.0587 - val_accuracy: 0.9823\n",
            "Epoch 53/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0381 - accuracy: 0.9900 - val_loss: 0.0622 - val_accuracy: 0.9823\n",
            "Epoch 54/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0396 - accuracy: 0.9887 - val_loss: 0.0616 - val_accuracy: 0.9823\n",
            "Epoch 55/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0363 - accuracy: 0.9887 - val_loss: 0.0604 - val_accuracy: 0.9831\n",
            "Epoch 56/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.9895 - val_loss: 0.0580 - val_accuracy: 0.9838\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0369 - accuracy: 0.9897 - val_loss: 0.0588 - val_accuracy: 0.9831\n",
            "Epoch 58/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0371 - accuracy: 0.9892 - val_loss: 0.0584 - val_accuracy: 0.9838\n",
            "Epoch 59/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 0.9897 - val_loss: 0.0613 - val_accuracy: 0.9831\n",
            "Epoch 60/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.9890 - val_loss: 0.0650 - val_accuracy: 0.9815\n",
            "Epoch 61/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0378 - accuracy: 0.9892 - val_loss: 0.0620 - val_accuracy: 0.9823\n",
            "Epoch 62/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0373 - accuracy: 0.9887 - val_loss: 0.0595 - val_accuracy: 0.9838\n",
            "Epoch 63/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.9897 - val_loss: 0.0624 - val_accuracy: 0.9831\n",
            "Epoch 64/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9897 - val_loss: 0.0598 - val_accuracy: 0.9831\n",
            "Epoch 65/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9892 - val_loss: 0.0590 - val_accuracy: 0.9838\n",
            "Epoch 66/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0386 - accuracy: 0.9892 - val_loss: 0.0603 - val_accuracy: 0.9838\n",
            "Epoch 67/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 0.0604 - val_accuracy: 0.9831\n",
            "Epoch 68/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0376 - accuracy: 0.9897 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
            "Epoch 69/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9895 - val_loss: 0.0577 - val_accuracy: 0.9823\n",
            "Epoch 70/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0364 - accuracy: 0.9892 - val_loss: 0.0594 - val_accuracy: 0.9854\n",
            "Epoch 71/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0375 - accuracy: 0.9902 - val_loss: 0.0588 - val_accuracy: 0.9831\n",
            "Epoch 72/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0385 - accuracy: 0.9879 - val_loss: 0.0586 - val_accuracy: 0.9838\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9892 - val_loss: 0.0600 - val_accuracy: 0.9831\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.9892 - val_loss: 0.0584 - val_accuracy: 0.9838\n",
            "Epoch 75/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0374 - accuracy: 0.9900 - val_loss: 0.0568 - val_accuracy: 0.9823\n",
            "Epoch 76/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0398 - accuracy: 0.9872 - val_loss: 0.0608 - val_accuracy: 0.9823\n",
            "Epoch 77/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0385 - accuracy: 0.9882 - val_loss: 0.0606 - val_accuracy: 0.9831\n",
            "Epoch 78/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0376 - accuracy: 0.9895 - val_loss: 0.0694 - val_accuracy: 0.9838\n",
            "Epoch 79/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0383 - accuracy: 0.9879 - val_loss: 0.0622 - val_accuracy: 0.9838\n",
            "Epoch 80/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0367 - accuracy: 0.9892 - val_loss: 0.0658 - val_accuracy: 0.9831\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0357 - accuracy: 0.9892 - val_loss: 0.0571 - val_accuracy: 0.9838\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0372 - accuracy: 0.9887 - val_loss: 0.0580 - val_accuracy: 0.9831\n",
            "Epoch 83/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.9897 - val_loss: 0.0618 - val_accuracy: 0.9823\n",
            "Epoch 84/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0360 - accuracy: 0.9887 - val_loss: 0.0684 - val_accuracy: 0.9831\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 0.9900 - val_loss: 0.0610 - val_accuracy: 0.9831\n",
            "Epoch 86/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 0.0642 - val_accuracy: 0.9838\n",
            "Epoch 87/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0356 - accuracy: 0.9897 - val_loss: 0.0610 - val_accuracy: 0.9831\n",
            "Epoch 88/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 0.0595 - val_accuracy: 0.9846\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0386 - accuracy: 0.9882 - val_loss: 0.0597 - val_accuracy: 0.9823\n",
            "Epoch 90/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0388 - accuracy: 0.9890 - val_loss: 0.0566 - val_accuracy: 0.9831\n",
            "Epoch 91/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 0.9869 - val_loss: 0.0588 - val_accuracy: 0.9815\n",
            "Epoch 92/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0371 - accuracy: 0.9895 - val_loss: 0.0640 - val_accuracy: 0.9823\n",
            "Epoch 93/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0359 - accuracy: 0.9897 - val_loss: 0.0599 - val_accuracy: 0.9846\n",
            "Epoch 94/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 0.0610 - val_accuracy: 0.9823\n",
            "Epoch 95/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0371 - accuracy: 0.9908 - val_loss: 0.0609 - val_accuracy: 0.9815\n",
            "Epoch 96/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0399 - accuracy: 0.9864 - val_loss: 0.0605 - val_accuracy: 0.9831\n",
            "Epoch 97/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9877 - val_loss: 0.0601 - val_accuracy: 0.9831\n",
            "Epoch 98/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0433 - accuracy: 0.9859 - val_loss: 0.0729 - val_accuracy: 0.9838\n",
            "Epoch 99/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0380 - accuracy: 0.9892 - val_loss: 0.0615 - val_accuracy: 0.9838\n",
            "Epoch 100/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0360 - accuracy: 0.9908 - val_loss: 0.0588 - val_accuracy: 0.9838\n",
            "Epoch 101/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0354 - accuracy: 0.9902 - val_loss: 0.0616 - val_accuracy: 0.9823\n",
            "Epoch 102/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0358 - accuracy: 0.9908 - val_loss: 0.0606 - val_accuracy: 0.9838\n",
            "Epoch 103/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0356 - accuracy: 0.9905 - val_loss: 0.0580 - val_accuracy: 0.9831\n",
            "Epoch 104/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0372 - accuracy: 0.9897 - val_loss: 0.0594 - val_accuracy: 0.9831\n",
            "Epoch 105/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.0643 - val_accuracy: 0.9838\n",
            "Epoch 106/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 0.9890 - val_loss: 0.0731 - val_accuracy: 0.9831\n",
            "Epoch 107/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 0.0578 - val_accuracy: 0.9831\n",
            "Epoch 108/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.9885 - val_loss: 0.0607 - val_accuracy: 0.9831\n",
            "Epoch 109/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 0.9887 - val_loss: 0.0647 - val_accuracy: 0.9823\n",
            "Epoch 110/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0384 - accuracy: 0.9892 - val_loss: 0.0594 - val_accuracy: 0.9838\n",
            "Epoch 111/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 0.9905 - val_loss: 0.0575 - val_accuracy: 0.9838\n",
            "Epoch 112/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0374 - accuracy: 0.9902 - val_loss: 0.0637 - val_accuracy: 0.9808\n",
            "Epoch 113/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0418 - accuracy: 0.9874 - val_loss: 0.0649 - val_accuracy: 0.9808\n",
            "Epoch 114/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.0591 - val_accuracy: 0.9838\n",
            "Epoch 115/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0424 - accuracy: 0.9867 - val_loss: 0.0653 - val_accuracy: 0.9815\n",
            "Epoch 116/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9859 - val_loss: 0.0774 - val_accuracy: 0.9800\n",
            "Epoch 117/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0413 - accuracy: 0.9879 - val_loss: 0.0636 - val_accuracy: 0.9831\n",
            "Epoch 118/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0360 - accuracy: 0.9902 - val_loss: 0.0584 - val_accuracy: 0.9838\n",
            "Epoch 119/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0351 - accuracy: 0.9905 - val_loss: 0.0614 - val_accuracy: 0.9838\n",
            "Epoch 120/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0351 - accuracy: 0.9902 - val_loss: 0.0605 - val_accuracy: 0.9831\n",
            "Epoch 121/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0357 - accuracy: 0.9897 - val_loss: 0.0627 - val_accuracy: 0.9838\n",
            "Epoch 122/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9895 - val_loss: 0.0593 - val_accuracy: 0.9846\n",
            "Epoch 123/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 0.0619 - val_accuracy: 0.9831\n",
            "Epoch 124/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9897 - val_loss: 0.0580 - val_accuracy: 0.9831\n",
            "Epoch 125/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9910 - val_loss: 0.0714 - val_accuracy: 0.9838\n",
            "Epoch 126/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 0.9887 - val_loss: 0.0611 - val_accuracy: 0.9846\n",
            "Epoch 127/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9882 - val_loss: 0.0609 - val_accuracy: 0.9831\n",
            "Epoch 128/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9890 - val_loss: 0.0601 - val_accuracy: 0.9823\n",
            "Epoch 129/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0393 - accuracy: 0.9890 - val_loss: 0.0612 - val_accuracy: 0.9831\n",
            "Epoch 130/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0447 - accuracy: 0.9872 - val_loss: 0.0684 - val_accuracy: 0.9808\n",
            "Epoch 131/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0460 - accuracy: 0.9867 - val_loss: 0.0690 - val_accuracy: 0.9792\n",
            "Epoch 132/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0495 - accuracy: 0.9838 - val_loss: 0.0592 - val_accuracy: 0.9846\n",
            "Epoch 133/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0396 - accuracy: 0.9885 - val_loss: 0.0642 - val_accuracy: 0.9823\n",
            "Epoch 134/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 0.0700 - val_accuracy: 0.9846\n",
            "Epoch 135/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 0.0622 - val_accuracy: 0.9846\n",
            "Epoch 136/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.9905 - val_loss: 0.0599 - val_accuracy: 0.9846\n",
            "Epoch 137/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0355 - accuracy: 0.9900 - val_loss: 0.0636 - val_accuracy: 0.9838\n",
            "Epoch 138/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.9905 - val_loss: 0.0619 - val_accuracy: 0.9838\n",
            "Epoch 139/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0353 - accuracy: 0.9892 - val_loss: 0.0625 - val_accuracy: 0.9831\n",
            "Epoch 140/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.0607 - val_accuracy: 0.9831\n",
            "Epoch 141/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9890 - val_loss: 0.0632 - val_accuracy: 0.9838\n",
            "Epoch 142/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0356 - accuracy: 0.9895 - val_loss: 0.0601 - val_accuracy: 0.9831\n",
            "Epoch 143/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0351 - accuracy: 0.9908 - val_loss: 0.0609 - val_accuracy: 0.9831\n",
            "Epoch 144/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.9877 - val_loss: 0.0654 - val_accuracy: 0.9838\n",
            "Epoch 145/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0357 - accuracy: 0.9895 - val_loss: 0.0587 - val_accuracy: 0.9838\n",
            "Epoch 146/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 0.0575 - val_accuracy: 0.9846\n",
            "Epoch 147/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.9895 - val_loss: 0.0769 - val_accuracy: 0.9800\n",
            "Epoch 148/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0398 - accuracy: 0.9879 - val_loss: 0.0626 - val_accuracy: 0.9838\n",
            "Epoch 149/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.0607 - val_accuracy: 0.9831\n",
            "Epoch 150/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0397 - accuracy: 0.9890 - val_loss: 0.0620 - val_accuracy: 0.9815\n",
            "Epoch 151/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.0572 - val_accuracy: 0.9831\n",
            "Epoch 152/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0391 - accuracy: 0.9887 - val_loss: 0.0587 - val_accuracy: 0.9823\n",
            "Epoch 153/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.0641 - val_accuracy: 0.9831\n",
            "Epoch 154/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9902 - val_loss: 0.0595 - val_accuracy: 0.9862\n",
            "Epoch 155/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0350 - accuracy: 0.9900 - val_loss: 0.0585 - val_accuracy: 0.9854\n",
            "Epoch 156/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.9890 - val_loss: 0.0662 - val_accuracy: 0.9831\n",
            "Epoch 157/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0392 - accuracy: 0.9879 - val_loss: 0.0852 - val_accuracy: 0.9785\n",
            "Epoch 158/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0410 - accuracy: 0.9887 - val_loss: 0.0635 - val_accuracy: 0.9831\n",
            "Epoch 159/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0357 - accuracy: 0.9897 - val_loss: 0.0628 - val_accuracy: 0.9838\n",
            "Epoch 160/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9900 - val_loss: 0.0586 - val_accuracy: 0.9846\n",
            "Epoch 161/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9897 - val_loss: 0.0612 - val_accuracy: 0.9831\n",
            "Epoch 162/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9902 - val_loss: 0.0596 - val_accuracy: 0.9846\n",
            "Epoch 163/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9879 - val_loss: 0.0570 - val_accuracy: 0.9846\n",
            "Epoch 164/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0372 - accuracy: 0.9892 - val_loss: 0.0580 - val_accuracy: 0.9838\n",
            "Epoch 165/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0357 - accuracy: 0.9902 - val_loss: 0.0566 - val_accuracy: 0.9838\n",
            "Epoch 166/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.9897 - val_loss: 0.0565 - val_accuracy: 0.9838\n",
            "Epoch 167/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.9897 - val_loss: 0.0591 - val_accuracy: 0.9846\n",
            "Epoch 168/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0354 - accuracy: 0.9902 - val_loss: 0.0648 - val_accuracy: 0.9831\n",
            "Epoch 169/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0346 - accuracy: 0.9905 - val_loss: 0.0594 - val_accuracy: 0.9862\n",
            "Epoch 170/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0349 - accuracy: 0.9902 - val_loss: 0.0602 - val_accuracy: 0.9854\n",
            "Epoch 171/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0346 - accuracy: 0.9905 - val_loss: 0.0617 - val_accuracy: 0.9831\n",
            "Epoch 172/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0354 - accuracy: 0.9887 - val_loss: 0.0593 - val_accuracy: 0.9862\n",
            "Epoch 173/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0371 - accuracy: 0.9882 - val_loss: 0.0587 - val_accuracy: 0.9854\n",
            "Epoch 174/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0377 - accuracy: 0.9887 - val_loss: 0.0609 - val_accuracy: 0.9831\n",
            "Epoch 175/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0379 - accuracy: 0.9887 - val_loss: 0.0596 - val_accuracy: 0.9854\n",
            "Epoch 176/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0351 - accuracy: 0.9897 - val_loss: 0.0611 - val_accuracy: 0.9862\n",
            "Epoch 177/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 0.0626 - val_accuracy: 0.9838\n",
            "Epoch 178/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.0598 - val_accuracy: 0.9846\n",
            "Epoch 179/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0359 - accuracy: 0.9900 - val_loss: 0.0581 - val_accuracy: 0.9838\n",
            "Epoch 180/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0366 - accuracy: 0.9895 - val_loss: 0.0594 - val_accuracy: 0.9838\n",
            "Epoch 181/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 0.9897 - val_loss: 0.0606 - val_accuracy: 0.9854\n",
            "Epoch 182/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0354 - accuracy: 0.9900 - val_loss: 0.0661 - val_accuracy: 0.9831\n",
            "Epoch 183/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0355 - accuracy: 0.9892 - val_loss: 0.0624 - val_accuracy: 0.9838\n",
            "Epoch 184/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0343 - accuracy: 0.9905 - val_loss: 0.0580 - val_accuracy: 0.9838\n",
            "Epoch 185/500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0364 - accuracy: 0.9887 - val_loss: 0.0616 - val_accuracy: 0.9815\n",
            "Epoch 186/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9892 - val_loss: 0.0666 - val_accuracy: 0.9831\n",
            "Epoch 187/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0357 - accuracy: 0.9895 - val_loss: 0.0592 - val_accuracy: 0.9854\n",
            "Epoch 188/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0350 - accuracy: 0.9902 - val_loss: 0.0630 - val_accuracy: 0.9831\n",
            "Epoch 189/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.0589 - val_accuracy: 0.9831\n",
            "Epoch 190/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0350 - accuracy: 0.9900 - val_loss: 0.0615 - val_accuracy: 0.9831\n",
            "Epoch 191/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0345 - accuracy: 0.9902 - val_loss: 0.0623 - val_accuracy: 0.9846\n",
            "Epoch 192/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0342 - accuracy: 0.9905 - val_loss: 0.0640 - val_accuracy: 0.9831\n",
            "Epoch 193/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0355 - accuracy: 0.9892 - val_loss: 0.0749 - val_accuracy: 0.9831\n",
            "Epoch 194/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0394 - accuracy: 0.9892 - val_loss: 0.0645 - val_accuracy: 0.9831\n",
            "Epoch 195/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0368 - accuracy: 0.9887 - val_loss: 0.0598 - val_accuracy: 0.9862\n",
            "Epoch 196/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0598 - val_accuracy: 0.9838\n",
            "Epoch 197/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0376 - accuracy: 0.9895 - val_loss: 0.0595 - val_accuracy: 0.9823\n",
            "Epoch 198/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0378 - accuracy: 0.9885 - val_loss: 0.0583 - val_accuracy: 0.9862\n",
            "Epoch 199/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0354 - accuracy: 0.9902 - val_loss: 0.0576 - val_accuracy: 0.9854\n",
            "Epoch 200/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0363 - accuracy: 0.9890 - val_loss: 0.0573 - val_accuracy: 0.9831\n",
            "Epoch 201/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0376 - accuracy: 0.9890 - val_loss: 0.0601 - val_accuracy: 0.9831\n",
            "Epoch 202/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 0.0633 - val_accuracy: 0.9838\n",
            "Epoch 203/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.9887 - val_loss: 0.0619 - val_accuracy: 0.9838\n",
            "Epoch 204/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0347 - accuracy: 0.9908 - val_loss: 0.0603 - val_accuracy: 0.9862\n",
            "Epoch 205/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0351 - accuracy: 0.9915 - val_loss: 0.0593 - val_accuracy: 0.9831\n",
            "Epoch 206/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 0.0579 - val_accuracy: 0.9862\n",
            "Epoch 207/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9910 - val_loss: 0.0595 - val_accuracy: 0.9854\n",
            "Epoch 208/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0359 - accuracy: 0.9910 - val_loss: 0.0638 - val_accuracy: 0.9823\n",
            "Epoch 209/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0406 - accuracy: 0.9890 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
            "Epoch 210/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0344 - accuracy: 0.9897 - val_loss: 0.0681 - val_accuracy: 0.9838\n",
            "Epoch 211/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9897 - val_loss: 0.0616 - val_accuracy: 0.9854\n",
            "Epoch 212/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0341 - accuracy: 0.9913 - val_loss: 0.0613 - val_accuracy: 0.9838\n",
            "Epoch 213/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0343 - accuracy: 0.9908 - val_loss: 0.0610 - val_accuracy: 0.9831\n",
            "Epoch 214/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0578 - val_accuracy: 0.9854\n",
            "Epoch 215/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0345 - accuracy: 0.9900 - val_loss: 0.0657 - val_accuracy: 0.9838\n",
            "Epoch 216/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9900 - val_loss: 0.0594 - val_accuracy: 0.9854\n",
            "Epoch 217/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9905 - val_loss: 0.0760 - val_accuracy: 0.9823\n",
            "Epoch 218/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0354 - accuracy: 0.9900 - val_loss: 0.0625 - val_accuracy: 0.9846\n",
            "Epoch 219/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.0601 - val_accuracy: 0.9846\n",
            "Epoch 220/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0345 - accuracy: 0.9897 - val_loss: 0.0660 - val_accuracy: 0.9838\n",
            "Epoch 221/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0343 - accuracy: 0.9905 - val_loss: 0.0612 - val_accuracy: 0.9854\n",
            "Epoch 222/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9900 - val_loss: 0.0618 - val_accuracy: 0.9846\n",
            "Epoch 223/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0344 - accuracy: 0.9902 - val_loss: 0.0619 - val_accuracy: 0.9854\n",
            "Epoch 224/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0343 - accuracy: 0.9915 - val_loss: 0.0607 - val_accuracy: 0.9854\n",
            "Epoch 225/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0356 - accuracy: 0.9895 - val_loss: 0.0729 - val_accuracy: 0.9838\n",
            "Epoch 226/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9892 - val_loss: 0.0717 - val_accuracy: 0.9815\n",
            "Epoch 227/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0358 - accuracy: 0.9902 - val_loss: 0.0671 - val_accuracy: 0.9838\n",
            "Epoch 228/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.9897 - val_loss: 0.0653 - val_accuracy: 0.9838\n",
            "Epoch 229/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0367 - accuracy: 0.9900 - val_loss: 0.0619 - val_accuracy: 0.9831\n",
            "Epoch 230/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0343 - accuracy: 0.9908 - val_loss: 0.0590 - val_accuracy: 0.9838\n",
            "Epoch 231/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0346 - accuracy: 0.9900 - val_loss: 0.0629 - val_accuracy: 0.9831\n",
            "Epoch 232/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9905 - val_loss: 0.0669 - val_accuracy: 0.9823\n",
            "Epoch 233/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0344 - accuracy: 0.9895 - val_loss: 0.0613 - val_accuracy: 0.9846\n",
            "Epoch 234/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0338 - accuracy: 0.9905 - val_loss: 0.0652 - val_accuracy: 0.9831\n",
            "Epoch 235/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0346 - accuracy: 0.9905 - val_loss: 0.0605 - val_accuracy: 0.9862\n",
            "Epoch 236/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0361 - accuracy: 0.9887 - val_loss: 0.0653 - val_accuracy: 0.9846\n",
            "Epoch 237/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0371 - accuracy: 0.9895 - val_loss: 0.0668 - val_accuracy: 0.9823\n",
            "Epoch 238/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9895 - val_loss: 0.0678 - val_accuracy: 0.9831\n",
            "Epoch 239/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.9890 - val_loss: 0.0595 - val_accuracy: 0.9869\n",
            "Epoch 240/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9913 - val_loss: 0.0602 - val_accuracy: 0.9854\n",
            "Epoch 241/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.9885 - val_loss: 0.0601 - val_accuracy: 0.9854\n",
            "Epoch 242/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0338 - accuracy: 0.9908 - val_loss: 0.0609 - val_accuracy: 0.9838\n",
            "Epoch 243/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.9879 - val_loss: 0.0646 - val_accuracy: 0.9838\n",
            "Epoch 244/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0425 - accuracy: 0.9874 - val_loss: 0.0621 - val_accuracy: 0.9846\n",
            "Epoch 245/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0425 - accuracy: 0.9859 - val_loss: 0.0728 - val_accuracy: 0.9838\n",
            "Epoch 246/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0410 - accuracy: 0.9890 - val_loss: 0.0822 - val_accuracy: 0.9800\n",
            "Epoch 247/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0383 - accuracy: 0.9879 - val_loss: 0.0703 - val_accuracy: 0.9838\n",
            "Epoch 248/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 0.9882 - val_loss: 0.0607 - val_accuracy: 0.9862\n",
            "Epoch 249/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9905 - val_loss: 0.0634 - val_accuracy: 0.9838\n",
            "Epoch 250/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0395 - accuracy: 0.9892 - val_loss: 0.0703 - val_accuracy: 0.9808\n",
            "Epoch 251/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0427 - accuracy: 0.9861 - val_loss: 0.0683 - val_accuracy: 0.9838\n",
            "Epoch 252/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 0.9895 - val_loss: 0.0750 - val_accuracy: 0.9808\n",
            "Epoch 253/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0377 - accuracy: 0.9882 - val_loss: 0.0727 - val_accuracy: 0.9831\n",
            "Epoch 254/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0346 - accuracy: 0.9905 - val_loss: 0.0617 - val_accuracy: 0.9862\n",
            "Epoch 255/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0336 - accuracy: 0.9902 - val_loss: 0.0639 - val_accuracy: 0.9831\n",
            "Epoch 256/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0343 - accuracy: 0.9905 - val_loss: 0.0642 - val_accuracy: 0.9838\n",
            "Epoch 257/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0340 - accuracy: 0.9902 - val_loss: 0.0648 - val_accuracy: 0.9854\n",
            "Epoch 258/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0341 - accuracy: 0.9910 - val_loss: 0.0614 - val_accuracy: 0.9862\n",
            "Epoch 259/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0342 - accuracy: 0.9900 - val_loss: 0.0605 - val_accuracy: 0.9823\n",
            "Epoch 260/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0598 - val_accuracy: 0.9846\n",
            "Epoch 261/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0337 - accuracy: 0.9905 - val_loss: 0.0600 - val_accuracy: 0.9854\n",
            "Epoch 262/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0351 - accuracy: 0.9897 - val_loss: 0.0638 - val_accuracy: 0.9838\n",
            "Epoch 263/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0610 - val_accuracy: 0.9862\n",
            "Epoch 264/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0346 - accuracy: 0.9905 - val_loss: 0.0640 - val_accuracy: 0.9831\n",
            "Epoch 265/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0351 - accuracy: 0.9900 - val_loss: 0.0607 - val_accuracy: 0.9838\n",
            "Epoch 266/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9892 - val_loss: 0.0602 - val_accuracy: 0.9869\n",
            "Epoch 267/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0360 - accuracy: 0.9895 - val_loss: 0.0660 - val_accuracy: 0.9831\n",
            "Epoch 268/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0366 - accuracy: 0.9897 - val_loss: 0.0838 - val_accuracy: 0.9792\n",
            "Epoch 269/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 0.9900 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
            "Epoch 270/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0405 - accuracy: 0.9885 - val_loss: 0.0673 - val_accuracy: 0.9838\n",
            "Epoch 271/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0345 - accuracy: 0.9902 - val_loss: 0.0603 - val_accuracy: 0.9838\n",
            "Epoch 272/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0353 - accuracy: 0.9905 - val_loss: 0.0600 - val_accuracy: 0.9862\n",
            "Epoch 273/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0354 - accuracy: 0.9892 - val_loss: 0.0642 - val_accuracy: 0.9838\n",
            "Epoch 274/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0343 - accuracy: 0.9897 - val_loss: 0.0609 - val_accuracy: 0.9838\n",
            "Epoch 275/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0347 - accuracy: 0.9892 - val_loss: 0.0615 - val_accuracy: 0.9846\n",
            "Epoch 276/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0339 - accuracy: 0.9913 - val_loss: 0.0622 - val_accuracy: 0.9854\n",
            "Epoch 277/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9895 - val_loss: 0.0616 - val_accuracy: 0.9846\n",
            "Epoch 278/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0369 - accuracy: 0.9902 - val_loss: 0.0629 - val_accuracy: 0.9838\n",
            "Epoch 279/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0368 - accuracy: 0.9895 - val_loss: 0.0635 - val_accuracy: 0.9862\n",
            "Epoch 280/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0341 - accuracy: 0.9897 - val_loss: 0.0675 - val_accuracy: 0.9838\n",
            "Epoch 281/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0340 - accuracy: 0.9910 - val_loss: 0.0608 - val_accuracy: 0.9862\n",
            "Epoch 282/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0339 - accuracy: 0.9905 - val_loss: 0.0639 - val_accuracy: 0.9831\n",
            "Epoch 283/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0337 - accuracy: 0.9913 - val_loss: 0.0664 - val_accuracy: 0.9831\n",
            "Epoch 284/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0341 - accuracy: 0.9910 - val_loss: 0.0642 - val_accuracy: 0.9838\n",
            "Epoch 285/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0335 - accuracy: 0.9915 - val_loss: 0.0630 - val_accuracy: 0.9846\n",
            "Epoch 286/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0336 - accuracy: 0.9902 - val_loss: 0.0672 - val_accuracy: 0.9831\n",
            "Epoch 287/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0333 - accuracy: 0.9910 - val_loss: 0.0638 - val_accuracy: 0.9831\n",
            "Epoch 288/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0349 - accuracy: 0.9897 - val_loss: 0.0610 - val_accuracy: 0.9838\n",
            "Epoch 289/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9895 - val_loss: 0.0639 - val_accuracy: 0.9846\n",
            "Epoch 290/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0341 - accuracy: 0.9908 - val_loss: 0.0640 - val_accuracy: 0.9862\n",
            "Epoch 291/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0334 - accuracy: 0.9910 - val_loss: 0.0634 - val_accuracy: 0.9823\n",
            "Epoch 292/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 0.9895 - val_loss: 0.0761 - val_accuracy: 0.9823\n",
            "Epoch 293/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0413 - accuracy: 0.9877 - val_loss: 0.0684 - val_accuracy: 0.9838\n",
            "Epoch 294/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0433 - accuracy: 0.9869 - val_loss: 0.0614 - val_accuracy: 0.9838\n",
            "Epoch 295/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.9897 - val_loss: 0.0656 - val_accuracy: 0.9838\n",
            "Epoch 296/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0336 - accuracy: 0.9897 - val_loss: 0.0698 - val_accuracy: 0.9831\n",
            "Epoch 297/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0359 - accuracy: 0.9895 - val_loss: 0.0656 - val_accuracy: 0.9831\n",
            "Epoch 298/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9902 - val_loss: 0.0574 - val_accuracy: 0.9838\n",
            "Epoch 299/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9890 - val_loss: 0.0624 - val_accuracy: 0.9831\n",
            "Epoch 300/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0358 - accuracy: 0.9887 - val_loss: 0.0715 - val_accuracy: 0.9823\n",
            "Epoch 301/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0365 - accuracy: 0.9887 - val_loss: 0.0802 - val_accuracy: 0.9823\n",
            "Epoch 302/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0375 - accuracy: 0.9890 - val_loss: 0.0666 - val_accuracy: 0.9831\n",
            "Epoch 303/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0417 - accuracy: 0.9879 - val_loss: 0.0794 - val_accuracy: 0.9823\n",
            "Epoch 304/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0375 - accuracy: 0.9892 - val_loss: 0.0644 - val_accuracy: 0.9838\n",
            "Epoch 305/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9900 - val_loss: 0.0562 - val_accuracy: 0.9869\n",
            "Epoch 306/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 0.9915 - val_loss: 0.0561 - val_accuracy: 0.9846\n",
            "Epoch 307/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0380 - accuracy: 0.9887 - val_loss: 0.0663 - val_accuracy: 0.9838\n",
            "Epoch 308/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 0.0594 - val_accuracy: 0.9838\n",
            "Epoch 309/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0334 - accuracy: 0.9910 - val_loss: 0.0599 - val_accuracy: 0.9838\n",
            "Epoch 310/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0334 - accuracy: 0.9913 - val_loss: 0.0625 - val_accuracy: 0.9854\n",
            "Epoch 311/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0331 - accuracy: 0.9915 - val_loss: 0.0611 - val_accuracy: 0.9854\n",
            "Epoch 312/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0329 - accuracy: 0.9910 - val_loss: 0.0574 - val_accuracy: 0.9869\n",
            "Epoch 313/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.0706 - val_accuracy: 0.9831\n",
            "Epoch 314/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0357 - accuracy: 0.9895 - val_loss: 0.0624 - val_accuracy: 0.9838\n",
            "Epoch 315/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0360 - accuracy: 0.9897 - val_loss: 0.0630 - val_accuracy: 0.9831\n",
            "Epoch 316/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 0.9895 - val_loss: 0.0653 - val_accuracy: 0.9838\n",
            "Epoch 317/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9872 - val_loss: 0.0620 - val_accuracy: 0.9838\n",
            "Epoch 318/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0821 - val_accuracy: 0.9815\n",
            "Epoch 319/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.9905 - val_loss: 0.0611 - val_accuracy: 0.9838\n",
            "Epoch 320/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0335 - accuracy: 0.9915 - val_loss: 0.0635 - val_accuracy: 0.9831\n",
            "Epoch 321/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0341 - accuracy: 0.9902 - val_loss: 0.0604 - val_accuracy: 0.9838\n",
            "Epoch 322/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 0.0587 - val_accuracy: 0.9862\n",
            "Epoch 323/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0335 - accuracy: 0.9913 - val_loss: 0.0570 - val_accuracy: 0.9846\n",
            "Epoch 324/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.0596 - val_accuracy: 0.9846\n",
            "Epoch 325/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0338 - accuracy: 0.9890 - val_loss: 0.0602 - val_accuracy: 0.9846\n",
            "Epoch 326/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9900 - val_loss: 0.0625 - val_accuracy: 0.9838\n",
            "Epoch 327/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0331 - accuracy: 0.9913 - val_loss: 0.0621 - val_accuracy: 0.9831\n",
            "Epoch 328/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.9905 - val_loss: 0.0655 - val_accuracy: 0.9831\n",
            "Epoch 329/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0356 - accuracy: 0.9895 - val_loss: 0.0628 - val_accuracy: 0.9831\n",
            "Epoch 330/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0345 - accuracy: 0.9897 - val_loss: 0.0667 - val_accuracy: 0.9831\n",
            "Epoch 331/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.9900 - val_loss: 0.0589 - val_accuracy: 0.9862\n",
            "Epoch 332/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0330 - accuracy: 0.9908 - val_loss: 0.0586 - val_accuracy: 0.9838\n",
            "Epoch 333/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0331 - accuracy: 0.9905 - val_loss: 0.0576 - val_accuracy: 0.9862\n",
            "Epoch 334/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 0.9902 - val_loss: 0.0582 - val_accuracy: 0.9854\n",
            "Epoch 335/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0346 - accuracy: 0.9897 - val_loss: 0.0598 - val_accuracy: 0.9869\n",
            "Epoch 336/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9892 - val_loss: 0.0589 - val_accuracy: 0.9862\n",
            "Epoch 337/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9897 - val_loss: 0.0588 - val_accuracy: 0.9869\n",
            "Epoch 338/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0335 - accuracy: 0.9902 - val_loss: 0.0618 - val_accuracy: 0.9846\n",
            "Epoch 339/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0330 - accuracy: 0.9908 - val_loss: 0.0634 - val_accuracy: 0.9831\n",
            "Epoch 340/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0337 - accuracy: 0.9900 - val_loss: 0.0639 - val_accuracy: 0.9823\n",
            "Epoch 341/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9900 - val_loss: 0.0616 - val_accuracy: 0.9838\n",
            "Epoch 342/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0345 - accuracy: 0.9897 - val_loss: 0.0659 - val_accuracy: 0.9846\n",
            "Epoch 343/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0334 - accuracy: 0.9900 - val_loss: 0.0703 - val_accuracy: 0.9823\n",
            "Epoch 344/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.0744 - val_accuracy: 0.9831\n",
            "Epoch 345/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0397 - accuracy: 0.9872 - val_loss: 0.0607 - val_accuracy: 0.9862\n",
            "Epoch 346/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0333 - accuracy: 0.9905 - val_loss: 0.0631 - val_accuracy: 0.9823\n",
            "Epoch 347/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0334 - accuracy: 0.9915 - val_loss: 0.0642 - val_accuracy: 0.9831\n",
            "Epoch 348/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0330 - accuracy: 0.9910 - val_loss: 0.0621 - val_accuracy: 0.9854\n",
            "Epoch 349/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0328 - accuracy: 0.9918 - val_loss: 0.0640 - val_accuracy: 0.9838\n",
            "Epoch 350/500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0347 - accuracy: 0.9900 - val_loss: 0.0632 - val_accuracy: 0.9846\n",
            "Epoch 351/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0346 - accuracy: 0.9910 - val_loss: 0.0685 - val_accuracy: 0.9838\n",
            "Epoch 352/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0356 - accuracy: 0.9897 - val_loss: 0.0800 - val_accuracy: 0.9815\n",
            "Epoch 353/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0384 - accuracy: 0.9885 - val_loss: 0.0728 - val_accuracy: 0.9823\n",
            "Epoch 354/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0379 - accuracy: 0.9892 - val_loss: 0.0700 - val_accuracy: 0.9838\n",
            "Epoch 355/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0395 - accuracy: 0.9890 - val_loss: 0.0688 - val_accuracy: 0.9854\n",
            "Epoch 356/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0402 - accuracy: 0.9867 - val_loss: 0.0630 - val_accuracy: 0.9838\n",
            "Epoch 357/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0351 - accuracy: 0.9910 - val_loss: 0.0631 - val_accuracy: 0.9838\n",
            "Epoch 358/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9895 - val_loss: 0.0598 - val_accuracy: 0.9854\n",
            "Epoch 359/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0357 - accuracy: 0.9900 - val_loss: 0.0600 - val_accuracy: 0.9838\n",
            "Epoch 360/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0713 - val_accuracy: 0.9838\n",
            "Epoch 361/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 0.9895 - val_loss: 0.0611 - val_accuracy: 0.9877\n",
            "Epoch 362/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0339 - accuracy: 0.9905 - val_loss: 0.0667 - val_accuracy: 0.9838\n",
            "Epoch 363/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 0.9910 - val_loss: 0.0623 - val_accuracy: 0.9838\n",
            "Epoch 364/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0338 - accuracy: 0.9905 - val_loss: 0.0646 - val_accuracy: 0.9831\n",
            "Epoch 365/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0620 - val_accuracy: 0.9838\n",
            "Epoch 366/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 0.9905 - val_loss: 0.0644 - val_accuracy: 0.9838\n",
            "Epoch 367/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0327 - accuracy: 0.9908 - val_loss: 0.0608 - val_accuracy: 0.9854\n",
            "Epoch 368/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0330 - accuracy: 0.9905 - val_loss: 0.0615 - val_accuracy: 0.9838\n",
            "Epoch 369/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0376 - accuracy: 0.9887 - val_loss: 0.0643 - val_accuracy: 0.9831\n",
            "Epoch 370/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 0.9882 - val_loss: 0.0633 - val_accuracy: 0.9869\n",
            "Epoch 371/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0337 - accuracy: 0.9915 - val_loss: 0.0642 - val_accuracy: 0.9846\n",
            "Epoch 372/500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0328 - accuracy: 0.9913 - val_loss: 0.0554 - val_accuracy: 0.9862\n",
            "Epoch 373/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0335 - accuracy: 0.9902 - val_loss: 0.0619 - val_accuracy: 0.9846\n",
            "Epoch 374/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0334 - accuracy: 0.9908 - val_loss: 0.0593 - val_accuracy: 0.9854\n",
            "Epoch 375/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0341 - accuracy: 0.9895 - val_loss: 0.0591 - val_accuracy: 0.9869\n",
            "Epoch 376/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0337 - accuracy: 0.9897 - val_loss: 0.0592 - val_accuracy: 0.9846\n",
            "Epoch 377/500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0323 - accuracy: 0.9910 - val_loss: 0.0578 - val_accuracy: 0.9854\n",
            "Epoch 378/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9892 - val_loss: 0.0593 - val_accuracy: 0.9862\n",
            "Epoch 379/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0357 - accuracy: 0.9892 - val_loss: 0.0646 - val_accuracy: 0.9854\n",
            "Epoch 380/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9905 - val_loss: 0.0666 - val_accuracy: 0.9831\n",
            "Epoch 381/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0328 - accuracy: 0.9918 - val_loss: 0.0614 - val_accuracy: 0.9869\n",
            "Epoch 382/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0336 - accuracy: 0.9905 - val_loss: 0.0618 - val_accuracy: 0.9869\n",
            "Epoch 383/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0327 - accuracy: 0.9897 - val_loss: 0.0603 - val_accuracy: 0.9846\n",
            "Epoch 384/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0343 - accuracy: 0.9895 - val_loss: 0.0613 - val_accuracy: 0.9854\n",
            "Epoch 385/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0328 - accuracy: 0.9910 - val_loss: 0.0690 - val_accuracy: 0.9831\n",
            "Epoch 386/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9900 - val_loss: 0.0691 - val_accuracy: 0.9831\n",
            "Epoch 387/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0360 - accuracy: 0.9887 - val_loss: 0.0690 - val_accuracy: 0.9838\n",
            "Epoch 388/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0337 - accuracy: 0.9900 - val_loss: 0.0605 - val_accuracy: 0.9838\n",
            "Epoch 389/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0347 - accuracy: 0.9895 - val_loss: 0.0649 - val_accuracy: 0.9854\n",
            "Epoch 390/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0336 - accuracy: 0.9892 - val_loss: 0.0614 - val_accuracy: 0.9854\n",
            "Epoch 391/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.0600 - val_accuracy: 0.9869\n",
            "Epoch 392/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0334 - accuracy: 0.9897 - val_loss: 0.0608 - val_accuracy: 0.9869\n",
            "Epoch 393/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0331 - accuracy: 0.9905 - val_loss: 0.0588 - val_accuracy: 0.9854\n",
            "Epoch 394/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0324 - accuracy: 0.9908 - val_loss: 0.0641 - val_accuracy: 0.9846\n",
            "Epoch 395/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0327 - accuracy: 0.9910 - val_loss: 0.0613 - val_accuracy: 0.9838\n",
            "Epoch 396/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.9918 - val_loss: 0.0642 - val_accuracy: 0.9846\n",
            "Epoch 397/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0326 - accuracy: 0.9910 - val_loss: 0.0582 - val_accuracy: 0.9862\n",
            "Epoch 398/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0333 - accuracy: 0.9902 - val_loss: 0.0598 - val_accuracy: 0.9862\n",
            "Epoch 399/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0337 - accuracy: 0.9902 - val_loss: 0.0616 - val_accuracy: 0.9869\n",
            "Epoch 400/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0327 - accuracy: 0.9908 - val_loss: 0.0626 - val_accuracy: 0.9854\n",
            "Epoch 401/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0354 - accuracy: 0.9897 - val_loss: 0.0631 - val_accuracy: 0.9854\n",
            "Epoch 402/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 0.9890 - val_loss: 0.0640 - val_accuracy: 0.9808\n",
            "Epoch 403/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0388 - accuracy: 0.9885 - val_loss: 0.0649 - val_accuracy: 0.9846\n",
            "Epoch 404/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9897 - val_loss: 0.0641 - val_accuracy: 0.9854\n",
            "Epoch 405/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0326 - accuracy: 0.9915 - val_loss: 0.0597 - val_accuracy: 0.9846\n",
            "Epoch 406/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 0.9910 - val_loss: 0.0651 - val_accuracy: 0.9838\n",
            "Epoch 407/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0324 - accuracy: 0.9908 - val_loss: 0.0612 - val_accuracy: 0.9869\n",
            "Epoch 408/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0325 - accuracy: 0.9920 - val_loss: 0.0613 - val_accuracy: 0.9862\n",
            "Epoch 409/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0325 - accuracy: 0.9910 - val_loss: 0.0638 - val_accuracy: 0.9838\n",
            "Epoch 410/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0326 - accuracy: 0.9910 - val_loss: 0.0661 - val_accuracy: 0.9838\n",
            "Epoch 411/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0341 - accuracy: 0.9897 - val_loss: 0.0628 - val_accuracy: 0.9862\n",
            "Epoch 412/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0328 - accuracy: 0.9905 - val_loss: 0.0615 - val_accuracy: 0.9862\n",
            "Epoch 413/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0321 - accuracy: 0.9910 - val_loss: 0.0670 - val_accuracy: 0.9838\n",
            "Epoch 414/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0334 - accuracy: 0.9910 - val_loss: 0.0688 - val_accuracy: 0.9846\n",
            "Epoch 415/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9902 - val_loss: 0.0796 - val_accuracy: 0.9808\n",
            "Epoch 416/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0386 - accuracy: 0.9879 - val_loss: 0.0791 - val_accuracy: 0.9815\n",
            "Epoch 417/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9877 - val_loss: 0.0636 - val_accuracy: 0.9862\n",
            "Epoch 418/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0408 - accuracy: 0.9867 - val_loss: 0.0623 - val_accuracy: 0.9831\n",
            "Epoch 419/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0365 - accuracy: 0.9892 - val_loss: 0.0642 - val_accuracy: 0.9815\n",
            "Epoch 420/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.0637 - val_accuracy: 0.9838\n",
            "Epoch 421/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.9879 - val_loss: 0.0856 - val_accuracy: 0.9831\n",
            "Epoch 422/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0348 - accuracy: 0.9905 - val_loss: 0.0660 - val_accuracy: 0.9846\n",
            "Epoch 423/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9892 - val_loss: 0.0652 - val_accuracy: 0.9854\n",
            "Epoch 424/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9887 - val_loss: 0.0687 - val_accuracy: 0.9869\n",
            "Epoch 425/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0331 - accuracy: 0.9913 - val_loss: 0.0624 - val_accuracy: 0.9831\n",
            "Epoch 426/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0338 - accuracy: 0.9902 - val_loss: 0.0579 - val_accuracy: 0.9854\n",
            "Epoch 427/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0345 - accuracy: 0.9902 - val_loss: 0.0605 - val_accuracy: 0.9854\n",
            "Epoch 428/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0367 - accuracy: 0.9902 - val_loss: 0.0753 - val_accuracy: 0.9838\n",
            "Epoch 429/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0327 - accuracy: 0.9905 - val_loss: 0.0651 - val_accuracy: 0.9862\n",
            "Epoch 430/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0345 - accuracy: 0.9887 - val_loss: 0.0720 - val_accuracy: 0.9846\n",
            "Epoch 431/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0334 - accuracy: 0.9905 - val_loss: 0.0626 - val_accuracy: 0.9862\n",
            "Epoch 432/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0335 - accuracy: 0.9902 - val_loss: 0.0701 - val_accuracy: 0.9846\n",
            "Epoch 433/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0347 - accuracy: 0.9902 - val_loss: 0.0643 - val_accuracy: 0.9869\n",
            "Epoch 434/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0318 - accuracy: 0.9910 - val_loss: 0.0661 - val_accuracy: 0.9846\n",
            "Epoch 435/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0314 - accuracy: 0.9915 - val_loss: 0.0654 - val_accuracy: 0.9862\n",
            "Epoch 436/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.0680 - val_accuracy: 0.9838\n",
            "Epoch 437/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0346 - accuracy: 0.9892 - val_loss: 0.0719 - val_accuracy: 0.9846\n",
            "Epoch 438/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0336 - accuracy: 0.9910 - val_loss: 0.0681 - val_accuracy: 0.9846\n",
            "Epoch 439/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0336 - accuracy: 0.9908 - val_loss: 0.0611 - val_accuracy: 0.9862\n",
            "Epoch 440/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0326 - accuracy: 0.9900 - val_loss: 0.0647 - val_accuracy: 0.9854\n",
            "Epoch 441/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0331 - accuracy: 0.9908 - val_loss: 0.0674 - val_accuracy: 0.9862\n",
            "Epoch 442/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 0.0896 - val_accuracy: 0.9808\n",
            "Epoch 443/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.0699 - val_accuracy: 0.9838\n",
            "Epoch 444/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9892 - val_loss: 0.0664 - val_accuracy: 0.9862\n",
            "Epoch 445/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0335 - accuracy: 0.9918 - val_loss: 0.0638 - val_accuracy: 0.9838\n",
            "Epoch 446/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9900 - val_loss: 0.0656 - val_accuracy: 0.9862\n",
            "Epoch 447/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0333 - accuracy: 0.9897 - val_loss: 0.0693 - val_accuracy: 0.9846\n",
            "Epoch 448/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0327 - accuracy: 0.9913 - val_loss: 0.0747 - val_accuracy: 0.9838\n",
            "Epoch 449/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0350 - accuracy: 0.9900 - val_loss: 0.0695 - val_accuracy: 0.9838\n",
            "Epoch 450/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0331 - accuracy: 0.9908 - val_loss: 0.0635 - val_accuracy: 0.9862\n",
            "Epoch 451/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 0.9923 - val_loss: 0.0615 - val_accuracy: 0.9869\n",
            "Epoch 452/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0328 - accuracy: 0.9905 - val_loss: 0.0629 - val_accuracy: 0.9846\n",
            "Epoch 453/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0343 - accuracy: 0.9895 - val_loss: 0.0659 - val_accuracy: 0.9846\n",
            "Epoch 454/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0388 - accuracy: 0.9877 - val_loss: 0.0751 - val_accuracy: 0.9831\n",
            "Epoch 455/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0357 - accuracy: 0.9892 - val_loss: 0.0674 - val_accuracy: 0.9854\n",
            "Epoch 456/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0372 - accuracy: 0.9892 - val_loss: 0.0854 - val_accuracy: 0.9831\n",
            "Epoch 457/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0345 - accuracy: 0.9895 - val_loss: 0.0679 - val_accuracy: 0.9854\n",
            "Epoch 458/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 0.0666 - val_accuracy: 0.9877\n",
            "Epoch 459/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 0.0640 - val_accuracy: 0.9823\n",
            "Epoch 460/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.9908 - val_loss: 0.0683 - val_accuracy: 0.9838\n",
            "Epoch 461/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0349 - accuracy: 0.9887 - val_loss: 0.0703 - val_accuracy: 0.9869\n",
            "Epoch 462/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0333 - accuracy: 0.9913 - val_loss: 0.0637 - val_accuracy: 0.9869\n",
            "Epoch 463/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.9900 - val_loss: 0.0632 - val_accuracy: 0.9862\n",
            "Epoch 464/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 0.9900 - val_loss: 0.0651 - val_accuracy: 0.9815\n",
            "Epoch 465/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.9887 - val_loss: 0.0628 - val_accuracy: 0.9854\n",
            "Epoch 466/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0345 - accuracy: 0.9887 - val_loss: 0.0615 - val_accuracy: 0.9854\n",
            "Epoch 467/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0316 - accuracy: 0.9908 - val_loss: 0.0717 - val_accuracy: 0.9831\n",
            "Epoch 468/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0332 - accuracy: 0.9902 - val_loss: 0.0764 - val_accuracy: 0.9846\n",
            "Epoch 469/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9892 - val_loss: 0.0654 - val_accuracy: 0.9862\n",
            "Epoch 470/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0341 - accuracy: 0.9902 - val_loss: 0.0638 - val_accuracy: 0.9877\n",
            "Epoch 471/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0323 - accuracy: 0.9905 - val_loss: 0.0624 - val_accuracy: 0.9838\n",
            "Epoch 472/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.0674 - val_accuracy: 0.9869\n",
            "Epoch 473/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.9900 - val_loss: 0.0713 - val_accuracy: 0.9846\n",
            "Epoch 474/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0327 - accuracy: 0.9905 - val_loss: 0.0785 - val_accuracy: 0.9838\n",
            "Epoch 475/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0358 - accuracy: 0.9887 - val_loss: 0.0753 - val_accuracy: 0.9846\n",
            "Epoch 476/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 0.9890 - val_loss: 0.0675 - val_accuracy: 0.9815\n",
            "Epoch 477/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0338 - accuracy: 0.9908 - val_loss: 0.0657 - val_accuracy: 0.9862\n",
            "Epoch 478/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 0.0669 - val_accuracy: 0.9862\n",
            "Epoch 479/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9902 - val_loss: 0.0728 - val_accuracy: 0.9846\n",
            "Epoch 480/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0327 - accuracy: 0.9908 - val_loss: 0.0710 - val_accuracy: 0.9831\n",
            "Epoch 481/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0323 - accuracy: 0.9902 - val_loss: 0.0619 - val_accuracy: 0.9869\n",
            "Epoch 482/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0329 - accuracy: 0.9915 - val_loss: 0.0648 - val_accuracy: 0.9854\n",
            "Epoch 483/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0321 - accuracy: 0.9913 - val_loss: 0.0656 - val_accuracy: 0.9862\n",
            "Epoch 484/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0317 - accuracy: 0.9918 - val_loss: 0.0705 - val_accuracy: 0.9846\n",
            "Epoch 485/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0330 - accuracy: 0.9900 - val_loss: 0.0735 - val_accuracy: 0.9831\n",
            "Epoch 486/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0329 - accuracy: 0.9900 - val_loss: 0.0792 - val_accuracy: 0.9838\n",
            "Epoch 487/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0332 - accuracy: 0.9897 - val_loss: 0.0676 - val_accuracy: 0.9862\n",
            "Epoch 488/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0316 - accuracy: 0.9913 - val_loss: 0.0654 - val_accuracy: 0.9854\n",
            "Epoch 489/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.0652 - val_accuracy: 0.9854\n",
            "Epoch 490/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0314 - accuracy: 0.9918 - val_loss: 0.0640 - val_accuracy: 0.9869\n",
            "Epoch 491/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0324 - accuracy: 0.9905 - val_loss: 0.0667 - val_accuracy: 0.9877\n",
            "Epoch 492/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 0.9913 - val_loss: 0.0625 - val_accuracy: 0.9862\n",
            "Epoch 493/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0319 - accuracy: 0.9902 - val_loss: 0.0659 - val_accuracy: 0.9846\n",
            "Epoch 494/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.0749 - val_accuracy: 0.9838\n",
            "Epoch 495/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 0.9897 - val_loss: 0.0721 - val_accuracy: 0.9846\n",
            "Epoch 496/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0366 - accuracy: 0.9897 - val_loss: 0.0650 - val_accuracy: 0.9846\n",
            "Epoch 497/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 0.0658 - val_accuracy: 0.9831\n",
            "Epoch 498/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9885 - val_loss: 0.0729 - val_accuracy: 0.9831\n",
            "Epoch 499/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.0691 - val_accuracy: 0.9869\n",
            "Epoch 500/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0388 - accuracy: 0.9882 - val_loss: 0.0813 - val_accuracy: 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# history에 저장된 학습 결과를 확인해 보겠습니다.\n",
        "hist_df=pd.DataFrame(history.history)\n",
        "hist_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "JVliuOnqWeps",
        "outputId": "ef6bcf4b-2465-4016-af58-c52c1ecd6725"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         loss  accuracy  val_loss  val_accuracy\n",
              "0    0.039486  0.988196  0.057622      0.982308\n",
              "1    0.040482  0.988196  0.061701      0.980769\n",
              "2    0.043978  0.987683  0.060785      0.983846\n",
              "3    0.043789  0.987683  0.068634      0.982308\n",
              "4    0.041860  0.986656  0.064860      0.983077\n",
              "..        ...       ...       ...           ...\n",
              "495  0.036567  0.989736  0.064960      0.984615\n",
              "496  0.032616  0.991275  0.065794      0.983077\n",
              "497  0.036790  0.988453  0.072908      0.983077\n",
              "498  0.041915  0.987426  0.069084      0.986923\n",
              "499  0.038767  0.988196  0.081293      0.981538\n",
              "\n",
              "[500 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2292922-7dbb-43ed-9615-854fc4fed161\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.039486</td>\n",
              "      <td>0.988196</td>\n",
              "      <td>0.057622</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.040482</td>\n",
              "      <td>0.988196</td>\n",
              "      <td>0.061701</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.043978</td>\n",
              "      <td>0.987683</td>\n",
              "      <td>0.060785</td>\n",
              "      <td>0.983846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.043789</td>\n",
              "      <td>0.987683</td>\n",
              "      <td>0.068634</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.041860</td>\n",
              "      <td>0.986656</td>\n",
              "      <td>0.064860</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0.036567</td>\n",
              "      <td>0.989736</td>\n",
              "      <td>0.064960</td>\n",
              "      <td>0.984615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0.032616</td>\n",
              "      <td>0.991275</td>\n",
              "      <td>0.065794</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0.036790</td>\n",
              "      <td>0.988453</td>\n",
              "      <td>0.072908</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.041915</td>\n",
              "      <td>0.987426</td>\n",
              "      <td>0.069084</td>\n",
              "      <td>0.986923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>0.038767</td>\n",
              "      <td>0.988196</td>\n",
              "      <td>0.081293</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2292922-7dbb-43ed-9615-854fc4fed161')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e2292922-7dbb-43ed-9615-854fc4fed161 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e2292922-7dbb-43ed-9615-854fc4fed161');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-45af1aee-6864-4af8-a842-23911b1954a7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-45af1aee-6864-4af8-a842-23911b1954a7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-45af1aee-6864-4af8-a842-23911b1954a7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "hist_df",
              "summary": "{\n  \"name\": \"hist_df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0027562903957545895,\n        \"min\": 0.03137129545211792,\n        \"max\": 0.04954224452376366,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          0.033931609243154526,\n          0.037631090730428696,\n          0.03410329297184944\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0012047335814635316,\n        \"min\": 0.9838337302207947,\n        \"max\": 0.9923017621040344,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          0.9899923205375671,\n          0.986143171787262,\n          0.988452672958374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005333562822674673,\n        \"min\": 0.05537943169474602,\n        \"max\": 0.08962933719158173,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          0.06668827682733536,\n          0.058436162769794464,\n          0.05905221775174141\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0015038546635018934,\n        \"min\": 0.9784615635871887,\n        \"max\": 0.9876922965049744,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.986923098564148,\n          0.9861538410186768,\n          0.9823076725006104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_vloss에 테스트셋의 오차를 저장합니다.\n",
        "y_vloss=hist_df['val_loss']\n",
        "\n",
        "# y_loss에 학습셋의 오차를 저장합니다.\n",
        "y_loss=hist_df['loss']\n",
        "\n",
        "# x 값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시합니다.\n",
        "x_len = np.arange(len(y_loss))\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
        "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "GmaXVGrQWlRy",
        "outputId": "8ef111b3-e203-46fb-fd00-e01c2b813618"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}